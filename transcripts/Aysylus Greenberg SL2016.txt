   Aysylus Greenberg.
   StrangeLoop 2016	
   Live captioning by Norma Miller @whitecoatcapxg
   
   >> Welcome, can everybody hear me? Thumbs up?
   >> Yes, awesome. So how many of you use a build system every day? Raise of hands. OK, let me see if I can get more of you to raise your hands. How many of you use Make? May have ever? Gradle? PIP, OK, so these are all examples of build systems and a build system is a software that automates compilation of your projects and produces binary, so today we'll talk about building a distributed build system at a Google scale. Work at Google on Google's infrastructure, today I'll talk about the projects that I work on as part of the development infrastructure team which I think is really, really cool. Currently I work on Google Drive's infrastructure.
   >> So what does it mean to build a distributed build system at Google scale. So let's unpack this title little bit little. So as we discussed a build system is a software that automates compilation of software and so in order to answer the question of what does build system actually do, let's look at the two similarities. So a build system will do a build or it can do a test, so what does that mean? So we'll have a project with some dependencies on the slide you see a, you know, just a template make file where on the left, right before the columns is the target, and then we see the dependencies on the right side and then the next line is the command you would execute to get the target.
   >> And so now the build system will take in this project with dependencies, and it would find the dependencies for you, so it means it has to be fed from a remote server and then once it found the den.dependencies and downloaded then, it would build this project with dependencies and it will emit binaries. So the build system will allow you to download the build artifacts or if you're interested in packaging them and putting them on a machine into production, the build system will allow you to do that as well.
   >> So say you are more interested in the results of the tests and you will run the tests and you will get the results of the tests, whether they passed or failed or which ones failed. So we discuss what build system does and what exactly it means to build and test, so let's talk about what it means to do this at Google scale and what kind of challenges are posed on the build system that is built at Google? So we have 30,000 engineers in over 40 offices so what that means is every single engineer needs to use a build system every day many, many times during the day and there is roughly 15,000 commits made by engineers every day, and about 30,000 commits made per tay by robots. What I mean by that it will have an automated script that will committing code to the repository and the reason we want do do this is open with 30,000 engineers there is so much work to be done so if we could automate that away so the human doesn't have to do the boring repetitive tasks and it could justing relegated to a script we'd like to do that as much as possible. So all these commits will have to go through the build system and the codebase is 3 billion of code. We need to build it, run the tests, and then ensure that all the failures are fixed before we commit the code.
   And there's roughly 5 million builds and tests that go specifically to the BuildRabbit system. BuildRabbit is the distributed build system that is part of the infrastructure and we'll look at it in this talk. BuildRabbit builds in on the or order of peta bytes. Some of the artifacts build system produces the artifacts, but the user may or may not care about retrieving them and all of this is done in one repository. So let's talk a little bit more about what that means and what kind of challenges it poses on a build system. So when Google open sourced a build system, it's internal build system and open source that is basal, there are a lot of comments and confusion, why would Google with such a large company with so many engineers, will use a monolithic repository and we already decided there was so much contention trying to push to that and didn't we already agree that it is causes so many problems? So let's look at that in a little bit more detail. I'm not going to go into too many details. And if you're very interested about how we do our source codes in one repository, there is a great talk by Rachael Potvin that she gave at a conference called at scale that I highly recommend you check it out. So to me, working in one repository is basically a tradeoff between having a repository of artifacts now from all the different repositories that you have, you just put them in something like Maven central or it's building from source, because now you have just one code -- one source code, and so you can just build from it, any time you need a particular revision that you're interested in and I assume that a lot of you are a lot more familiar with what it means to work in distributed code bases, right? Like many different code repositories? So today everything is a tradeoff but today specifically I'd like to talk about the advantages of working in one repository, because I assume that you're a lot more familiar with all the disadvantages of working in many different repositories. So first and foremost, when you have a monolithic repository, that allows you to have a lean revision history. Every single commit is an atomic change to the codebase and so the history is linear so there's not really confusion about what happens what and having to worry about interlevering a causality. And this also allows us to have extensive code sharing and reuse.
   Because everything can be cross-referenced. If I'm interested in deprecating my API and I want to figure out all the use sides in the functions of the API, I can look at the codebase and figure out if it's using it correctly or whether it's using it at all.
   It also makes it possible to do large-scale refactoring. So if we decide that some base library that we're all dependent on, for instance manipulating files, needs to be deprecated because we know a better approach to doing things. We can actually do things across all the codebase and we can do this in a lot more informed fashion and we can find all the different instances and just replace them. It's a lot easier to collaborate ha cross teams now, so if I'm working on some new piece of code and wondering what's the correct better use of the API, what the best practice is, I can look at other instances of uses of it and I can figure out and even look at the history of how the team progressed towards a better implementation or better use of the API, and I can learn from their mistakes and from their successes.
   And also, if I'm interested in changing something in a library, I can just send a commit to the team, send it for review, and see what they think if that aligns with their goals and have that submitted as soon as possible. It also simplifies dependency management, so as we talked about there's only one single source of truth and so there's no confusion about the authoritative version of the file, when you have it in a central repository if data gets corrupted or if there are many multiple versions of the same artifact, it could be confusing, like is this the one that the team actually wants me to use or should I use the older one? What happens if I use not the latest version of it? Would that have some issues that I should be aware of? Whereas here it's a lot more clear, we can just specify that we depend on something and let the team decide what is the acceptable version that I should use and there's no need to fork shared libraries. Now, if I need some features, the chances are some other team or some other engineer might need the same feature from the library. And finally, we still have a way to distinguish between work in progress version that say the library team is working on and the release version, because we use something like a code components for library releases, which is similar to git, subtree or git subcomponents, where basically the team can market, OK, this is the release version, everyone should use this one and this is a work in progress, we're still trying to figure out if it's something we can use later.
   With respect to the build system, this allows for predictable repeatable builds from source, so I know what it looks like in a particular revision and I can build the same target multiple times, and compare their outputs, their artifacts. If it happens that a different code path was taken through my build system, I should be able to -- I should not assume the results. The user shouldn't actually get to different binaries, just because they took a different code path and I had to route them elsewhere, it shouldn't actually cause any difference in output and so it's also easier for me as a build engineer to detect and that prove that there are some differences,.
   And we still have a lot of optimizations to avoid compiling the same artifacts. The biggest worry is if we don't have a central repository of artifacts, do we have to recompile those old dependencies again and again? No, we don't, because we can just detect that something has already been compiled and so we can still have very fast build.
   And this also allows to decouple each process as much as possible. For example if I'm depending on a library API and there is a bug in it but I need to release my code because my users are relying on me, I could just ask the team to tell me which pick I should commit and proceed with my release while they verifying everything on their end works as intended and release it on their own schedule and make sure it goes through all the processes that they have in place.
   So we discussed what the build system does, we discussed some of the challenges, the scale and some of the things that the build system should support when it works eat H. at Google scale. So let's talk about the distributed build system. How many people in the audience work with a distributed build system?
   I see a couple of hands, and you might actually it be using Travis CI, maybe for your continuous integration testing? OK, so distributed build system does very similar things to that. It's basically what the build system does, is instead of now being on your desktop and you use like command line interface, now you send the request to a remote server, and many of the companies don't really need to have a distributed build system built in house and here's why. So let's look at how build system evolves and when the need arises to have a distributed build system.
   So we'll have a person working on a project that they experimented with, and hoping something good will come out of it, and they will be building on their desktop using whatever build system that you're using at the time and suppose that they're machine learning experts and they will have a dependency on tensor flow so their build system will have a way to download that on their desktop and build that and compile that and so on. So now as the project becomes more successful, a whole team would be assigned to this project and now this team will set aside a box on the side and it will do its build and testing and all the continuous integration testing on this box.
   Now, a different team might have the same dependency on tensor flow if they work all in a machine-learning company and so now both teams have to have a box aside that they still have to worry about updating, maintaining, making sure that all the bug fixes are picked up in the system and you know, have a bigger company with more teams, every single team has to do this. So it poses an overhead for machine learning maybe something not their area of expertise and they don't really need to worry about it, but they have to, because there's not an infrastructure for them where they can just dedicate their builds.
   So that's where a team like my, build app, will step in where basically instead of having different teams having to maintain their boxes, they would just be in all one shared infrastructure, so a BuildRabbit will where teams can send builds and tests and they can focus an what they do best, and what they love, which is their machine-learning projects.
   So let's talk about BuildRabbit now, which is the distributed build system as Google which is part of the development infrastructure team. Build rabbit the way I like to think about it is basically Bazel in the cloud. And we'll see why I call it in the cloud, so in order to answer the question of what does BuildRabbit do, let's see where it fits in the you know the build stack. So BuildRabbit sits very nicely in the center of the build and test stack and it would be used in something like continuous integration system in order to be able to run the test and the release infrastructure will be able to use it to build their artifacts and use their artifacts to be deployed to production and all those orange boxes basically say that this is part of the developer infrastructure tooling, develop infrastructure to a bigger team. Now, other Google engineers and teams might want to do this and so they're marked in a different color. And then integration testing might want to do this, so basically building the services and bring up those instances and then running integration tests against them and built rabbit in turn will have dependencies on a source system, so we need to talk to the source system to figure out what is the state of the source system at the particular revision. If it was committed it would just pull it directly and if it hasn't been committed yet, then would use the overly with the user-proposed changes and now that it has the local environment set up, it will call into , blaze, which is Google's internal system and it would put the artifacts into the build artifacts storage and build rabbit will know how to retrieve those ar facts and give them back to the user. So nablas as I mentioned has been released as Bazel and BuildRabbit it doesn't do the test itself, it delegates it to Bazel or blaze, and all it does is provides all this distributed layer on top of that. It worries about what happens if machines fail. What happens if your connection breaks from the machine that it allocated for the work and so on.
   OK, so now that we're on the same page about what distributed build system does, let's talk about what it means to be building it at a Google scale.
   So the build system went through a transition where it started off with one architecture, and then we started hitting the limits and we had to redesign the architecture. And it started out with a very simple client server architecture which I like to think of as push architecture. This client library will send a request to the BuildRabbit scheduler which basically acts as a load balancer and then it would get the response back for which worker it should talk to and the client will request a worker to execute its workload, build a test request, and then it would establish the connection where over this connection it would get all the build artifacts and the build progress information. So this works really well for many years, while we were figuring out would this distributed build system need to do for the company and for the engineers, what is its use case? So the problems that we started running into is that so the client library was very thick, and indeed, it had a lot of logic related to routing and utilization of resource, the scheduler had some of that knowledge, too, but the client library needed to understand the transients and implied services inside the system. And also the client library if the connection broke between the scheduler and the worker, what that means is that are some of the big projects and big binaries with lots of dependencies. That meant that we had to trial over again and we lost all the progress that we had made and so this was suboptimal and we moved to something that we called build service where basically we moved to the pull model, where before the user connected to directly to the worker and pushed the work onto and say handle this build test request, now instead the worker pulled the work off of the persistent queue. So the user will put its build test request onto the persistent queue and it will be just, you know, an event-based connection, and so now when BuildRabbit worker has resources available is up and healthy, it would pick up this work and so it would get through an RPC from a persistent queue and now there were two different types of outputs that we were producing, there was the build artifacts, so the binaries that were built and the build progress information. And so once the BuildRabbit worker produced those, it would produce the built artifacts and put them one by one as it was compiling and linking the dependencies, and if we were running a test, which of the tests failed and which of them succeeded and how far are we into our completion of our request?
   And so at this point, if the user was interested in getting the build progress information, then they could connect to it, and if either the service that provides the progress information or the user go away, that's OK, because it doesn't go anywhere. And so the user can come back and figure out what happened in between its disconnecting.
   And similarly for build artifacts, P if the user cares about getting the build artifacts, then it would connect to the build artifact service and get those.
   And so all the four components, in green, would basically constitutes the build service. At this point, the old facade, the API is still the same, the types of things the user wants to accomplish with the build service are the same, but then we changed all the backend, we changed how we do this for better robustness of the system and so comparing them side by side, this is what we had. In the old architecture, the user would push the work to the BuildRabbit worker directly and then in the new architecture, the BuildRabbit work would pull the work off of the persistent queue, and in the meantime worker could query whether the build was executing, what's going on without losing that progress if it lost a direct connection.
   So the interesting thing is how the control logic happens, right? In the previous architecture, we had resilience logic in the thick client library, where you needed to understand a lot of the details of what it means to have a transient or permanent failure, did the user send a bad request for a file that would never compile or did the worker go away and then it stopped executing the work and that's what happened and the user should just retry.
   Whereas in the new architecture, now we have resilience logic which is centralized and I know it doesn't look like it's centralized because now it's spread over even more boxes but conceptually, now all the build service owns all the resilience logic and the user just sends simple requests for putting the work on and requesting some of the output.
   And so we needed to move from one architecture to the new architecture, from the old to new, and there were many challenges associated with this. So as I mentioned previously, BuildRabbit sits in the middle of the building test stack. We just can't ask users to stop building and testing, that's just not going to happen. And so what we needed to do is we needed to migrate from the old architecture to the new architecture and we needed to do this with no down time. Down time was just a luxury that we couldn't afford and so in the next couple of minutes I'd like to talk to you about some of the key considerations that allowed us to do this successfully.
   I'm not going to go into too many details but if you're interested I've given a talk about this at Midwest I/O, specifically how we pulled off migration with zero down time.
   These are some of the key considerations in case you're doing migration any time soon and you need to know more about it. So first and foremost we needed to ensure that we needed to migrate backends first. And the reason for this is we started to hit limitations of architecture where we couldn't support quite as many users as we like and we couldn't support the load that they were putting on us so we needed to ensure that all of this new backend in the new architecture can stand the expanded loads. And also what that meant is we need to leave the old facade up because we already gave out our client libraries to our users and so they are relying on particular functionality that we promised the build system does, and so we needed to do this with no user-visible changes whatsoever. Because they still expected the command to work exactly the same way it used to work and the test command works exactly the same way it used to, would and they don't really care now whether they had multiple services doing the things it used to. All it needs to be is invisible or even better experience.
   And so we needed to focus on the mixed mode and previously because we gave out our client libraries to users, we promised they'd be supported for some amount of time because it's unreasonable for users to keep up with our release cycle. We would like to release as much as possible, babu we can't demand users to rebuild their binaries when they do this, because they have their own system concerns to worry about and so what that meant is we needed to bridge between the old architecture and new architecture and everything in between as we were transitioning from one to the another. So we understood very well what the old architecture was like, we operated for some time we had a much better understanding of what it would do under what conditions. In the New York architecture we thought a lot about it and we understood where it needed to go. Now we needed to ensure we could support all the transition periods in between and so that's where most of the energy and focus went to make sure that in between the transition we still support all the old client likes and still provide the same functionality, but now they work Seamlessly with the new backends as they came up. We also needed to target launch-friendly targets. And so what we needed to do is we needed to find specific users that would benefit most from using our new system, and so we could work with them directly and have their help to tell us if something looks bad on their end and might not look bad on our end to know that there is a problem.
   And so what that meant is that for the three different jobs, it was actually three different launch-friendly clients. There were some clients that only cared about getting the build artifact. There were some clients that didn't really care about any of that and they just wanted the progress information of the build and there are some clients that really needed the upgraded execution engine to be up to their needs, and so this ended up being three different sets of launch-friendly clients and so what that meant is we needed to decouple launch of services. We couldn't rely on any one of the new backends to be up and so because we also had three different sets of users that are launch-friendly users, we needed to ensure that for any of the backends that we were bringing it up, they relied on none of the backends being up and running and also all of them being up and running and all of the in-between stages. And it was essential for us to get maximum visibility into our system state. It's an effort to get the perfect laser view into what is a happening at all times. We needed to launch something and not just not have anything. And we needed our metrics up of course and we needed to be able to query at any point in time and ask the system, is the system doing well? And if not, what is going wrong.
   We could dump everything into info logs, right and query them whenever we suspected something was going bad, but then if something was going bad, good luck finding what's actually going wrong and it was a lot of balancing between you know, what goes into info logs making sure that only essential information goes there, but also making sure that not just errors show up in the logs, but that successes also show up in the logs so we could know exactly which milestone we're at and where exactly our system is stuck.
   And we had to practice the launches a lot. What that meant is that every single person needed to be in loop, all the stakeholders needed to be in the loop about what's going to be the next step. Everybody's expectations need to be set correctly so that they knew what to expect and that meant lots and lots of practice launches, that meant making sure that everyone is on the same page about what's happening next and which team should start paying more attention to ensure that their part is covered correctly and so on. And so we created a lot of different checklists, where -- well, we created one long checklist that basically outlined exactly what steps we needed to take and also engineering resilience for people being able to get online when they wanted or people being able to leave. And that I also meant that we needed a solid roll-back plan, if we were only three out of say 100 steps of the launch in, what does it mean if things go back, how do we roll it back and sometimes it's as easy as taking the reverse of the operation that you did and sometimes that meant that the roll-back was not trivial and we needed to ensure that the right systems go back up act the correct time and so on. Otherwise the system will get confused and it would be in the mixed mode that we didn't foresee that we would be in.
   So a lot of you probably most likely might not work on distributed build system, but I wanted to show you a couple parting thoughts that hopefully you'll be able to use when you work on system and as you're building large-scale systems, not necessarily full-blown distributed systems, but any kind of system as you're planning the migrations and as you're planning evolutions of architectures.
   And specifically I would like to focus on distributed versus centralized control plane. So control plan is just a fancy way of saying all the control logic of if this happens, what's next. And so specifically let's talk about the distributed versus centralized control plane. Let's discuss it a little bit more.
   So with distributed control, that's what happens when we had an old architecture where our design was -- works well for one machine, so for the CLI tool on desktop and then we scaled it to work on many different machines and be able to send a build to remove server, many different remote servers, and so a lot of the logic ended up living in the thick client library that we gave out to our users and we needed to support all the different versions of the client libraries that we gave out, and what that meant is that it was very -- well, comparatively easy to scale that up to many machines, but then the bigger overhead was during the evolution of the system, because now we had all this overhead, we needed to keep in mind all the different previous implementations that we supported and make sure that previous client libraries that relied on them still work as expected.
   The opposite of that is having centralized control which is what happened when you had the build system created, and so it was a distributed design from the very start. We anticipated exactly which component would be responsible for what, and what happens when one goes down and how do we compensate for that, and what does fault tolerance mean in that context and the bigger overhead was in the design phase, we learned a lot from our previous lessons, but still, we needed to make sure that we design it correctly because once the design starts getting implemented and we realize that we haven't considered something major, that would be a big problem, that's essentially just throwing away all the work. On the opposite -- the upside of that is that now we're able to evolve our architecture, evolve our implementation a lot more quickly because we own a lot of the control logic in our system and our user has a very simple way of interacting with our system, and so things that we do can only improve it, but they don't have to worry about the details of implementation and they don't really have a client library anymore, they'll have a very much thinned-out version of the library.
   So this is all I have for today about how we built a distributed build system at Google scale. I believe we have a few minutes so I'd love to take your questions.
   [applause]
   >> Thank you. Yes?
   >> Go ahead.
   Just to make sure I understand, you are you saying Google have ways of Android servers ... ...
   >> So the question is, as I mentioned we have a monolithic repository, does that mean our search and all of our different products are all in the same code repository. Yes, that is what I meant, yes, that is correct. Which is will actually pretty fun sometimes to browse around and see how you know, people use and implement certain things. It was like oh, I didn't realize that team did that.
    AUDIENCE:  So with the different languages in your build repo, are your clients putting in the event that goes into your persistent queue what their build software is if it's made for Gradle or whether that come from the client?
   >> Yes, so the client, it's a polyglot codebase, do the clients just put the request? So the request is abstracted away. So it's not language-specific. We used one build system which was open sourced as Bazel. So all the major languages that Google supported like java, C++, Python, Go, and I think there are a couple of others, and so all of them, now irrespective, so we don't have different like Maven and Pip or something talking together, now all the clients use one language, one protocol, with which they express it, but then the codebase, their source code will be in different languages and we can do that and we can compile it and build it. OK, go ahead.
   >>
    AUDIENCE:  So you're creating at least some elements of a tool chain which you're distributing at scale, presumably, could you talk a little bit about how you do that with 30,000 developers when you're trying to push out the tools that you're create snag.
   >> OK, so the question is it could I talk a little bit more about since we're distributing the client library to all the different engineers, how do we do that. So once we compile the client library, we put it somewhere, we have a way for clients to pick up the latest client library if they're interested, and it's basically it's a repository of packages where they can pick it up and use it like just by rebuilding the binary and pointing to the latest. So now -- so that was in the old architecture. With the new architecture, the idea is actually to simplify that a lot more, where there are just commands they can send and so we can basically extend this experience of submitting the command and that could go into the build system. But the basic idea is, you know, once we have the client library that we would like our users to use, we put it somewhere and we tell them, any time you want to update your library, you specify the version number, and then it will get picked up, so we have like a whole different part of the build infrastructure, develop infrastructure that deals with that.
    AUDIENCE:  What's the operating system on the build machines?
   >> The question is it what's the operating system on build machines. I believe it was Linux-based.
   >> They're all identical?
   >> The question is, are they all identical? I mean it's a tough question, I mean like -- on the surface, they were all Linux-based operating system, but the devil is in the details, and how we do all the upgrades, and how we, you know, make patches to all the underlying things that go into the operating system.
   >> Yes, go ahead.
   >> [inaudible]
   >> OK, so the question is, for the source code repositories, is that git or something else? So Rachael Potvin goes into a lot more detail in her talk at the conference called at scale. It was based on per force, but now we have our own, which is called piper, so you can learn a lot more about that from her talk.
   >> Let's see, yes, I see a hand raised.
   >> Is your build -- if I find a build, is it always going to get -- [inaudible]
   >> So the question is it, is the build system stateless, and so if you send two different requests, would it preserve some state in between those two requests?
   >> No, if I give a request today and a request tomorrow --
   >> Right, so to clarify so two different requests are given, would it produce the same results? So we strive for repeatable builds as I mentioned earlier and so what that means is if the request is to build at a given revision in the source code and all the different variables are the same, it will produce the same results, so the build outputs had been the same. The artifacts must be the same, otherwise it's hard to tell, like, which of the versions of the build have produced that is which one is correct. Which one should I rely on so we do provide repeatable builds.
   >>
    AUDIENCE:  Does that mean that Google keeps their own copies of any -- open source software?
   >> So the question is, does Google keep internal delinquencies internally the dependencies that it might have on open source software. I can't talk about that today, but there's a lot of different security consequence related to that, and because our model is just not to have, you know, is a central repository of artifacts, but to build from source, then that influences a lot of our decisions instead of saying just lining in just download at a given version. Any other questions.
   So the question is does build system build iOS and and Android. So the build system that builds it is blaze and anything that blaze supports, build rabbit supports.
   >> Any other questions? I think I might be able to take just one.
   >> Oh, the -- if my project was in the library, can you [inaudible]
   >> So the question is, when defining a project, do we define the artifact that it depends on or do we define the actual source code. So I showed you earlier a make file where basically it's like the target defined and then how to produce the target, so the build language, you can it actually shows an example of a build file, it would basically specify very similarly a target and they those list of delinquencies. And those are also targets themselves and the build file specifies how to generate those and so on and so by specifying that, the build system will be able to say, have I built this target before and if I have, then they will be able to just link that directly without having to compile that. So it's all based on the target, we don't store separate repository of artifacts, but ifs a performance optimization.
    AUDIENCE:  Like I come in tomorrow and the library has been deleted. My code I'd probably want to specify like I want to build a previous version, right?
   >> Right, so then that part is about the components that I talked about, so the question was about, you know, if the library gets updated, some dependency gets updated how do I make sure that I have the latest one and that's something that we gave the power to the library teams where they can now specify and then the source system will be able to handle that correctly, and then we'll BuildRabbit will be able to create the local view of what the source code looks like at that revision and so if the library team releases something and in the morning I come in, it will get picked up. If I'm building at the version that is past the time when they built it. Great, so that's all the time I have today, but I'll be around here, if you all want to get donuts later and ask me questions as we go get donuts. Thanks for coming. Thank you.
   [applause]
