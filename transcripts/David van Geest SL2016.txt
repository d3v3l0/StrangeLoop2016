   David Van Geest
   StrangeLoop 2016.
   Live captioning by Norma Miller @whitecoatcapxg
   
   >> Hi, everyone. Thanks for coming out today, my name is David van Geest, and today we're talking about distributed task scheduling at PagerDuty.
   So first a little bit about me. I'm a software engineer on the core team at PagerDuty and it's composed of these fine people on the right here. The team is mostly in Toronto, but the company has offices in San Francisco and New York as well.
   >> One thing I want to emphasize is all of the ideas I'm talking about today, we worked on together. They're not my own ideas, I'm just the guy lucky -- or maybe unlucky -- to be standing right here. In particular, Lexie, who's on the left there, did much of the design work for this project, so kudos to him.
   So do we have any PagerDuty customers? The room? I see some hands. OK, awesome.
   So you know what we are, but for those of you who are not familiar, we are a software as a service company, and we do IT incident management. So what that means is that we aggregate data from monitoring systems like new relic or Datadog or Splunk, and we create data based on incidents that you set up. So an incident might be something like the database server crashed so when an incident is create alert people who need to know about it through push notifications or phone calls or SMAS and these notifications usually go to someone who's on call. So there's on-call scheduling built into the system. So long story short, our customers rely on us to let them know when something is wrong with their systems. And this is a really big responsibility, and one that we take very seriously.
   So because we take this responsibility so seriously, we think a lot about reliability and up time. And the most important piece of this is the notification pipeline. So if something is wrong in your system, you need to know about t and there's no excuses. So to achieve high availability in the face of catastrophic failure, our infrastructure runs in three different data centers and everything that we build is expected to survive the failure of one of those data centers. So keep this requirement in mind as we go through the talk. When I talk about a data centers failing this is a really big thing. We need to handle it well. So why are we here? Today I'll introduce the problem, I'll talk briefly about what the old solution warnings then I'll describe our new solution and this is the one built with Akka, Kafka, and Cassandra. After that we'll spend a good portion of our time talking about how the team used those three technologies to solve some interesting distributed systems problems. Finally we'll talk about the results of our project.
   OK, so the problem: In terms of the product, here are some use cases that we're trying to solve. So our system supports many types of notifications. If you're going on call, you might want some warning about that and maybe you want it two days ahead of time, so you remember, you know, don't party too hard. And then when you're actually doing your on-call shift you need to be notified that an incident happened. So we have very customizable rules for this, maybe you want a phone call immediately but you want a push notification one minute later to make sure that you're actually awake and out of bed. Another use case we have is retrying notification delivery. So we have redundant SMS providers, but what if they're all down for some reason? We need to schedule a retry in that case. So those are specific examples, but actually what we have is a very generic problem. We have these arbitrary chunks of Scala code which we'll call tasks and they need to run in arbitrary time and this code might do anything, it might be synchronous, it might be asynchronous, it could be scheduled from one second from now or it could be scheduled from one year from now. At the same time, it has to be tolerant of very dynamic infrastructure, so as I said, we currently run in three data centers, but it could be four next year or we might have to add or subtract ten machines and some from our data services. But despite all this, we need all our tasks to run in order.
   So ordering is a complex thing. Let's define it a bit more exactly for the purposes of the system.
   So we're going to say that task one is only guaranteed to execute before task two if these three things are true. Number one: They have the same ordering ID, that is, a developer said these things are linked and I want their ordering to be defined. No. 2, time of task 1 is before task 2. That makes sense and last thing the schedule task is completed before the call to schedule task 2 and that last bit is a little tricky. Let's say we have two tasks and they're scheduled very close together and they're scheduled for a second from now, so task 2 might execute first if the scheduling call completes before it does for task 1.
   But we have some room to make things easier as well, so tasks only need to be executed within seconds of their scheduled time. We're not talking about millisecond precision. We can also delay task execution arbitrarily to preserve our ordering. Also the tasks are item potent and this is a very important point. If you run them a second time nothing is going to happen. Lastly our tasks never fail, but they do sometimes and we have to be prepared for that occasional instance.
   Maybe this is just a simple matter of programming, you can just knock it out in an afternoon. Well, let's find out.
   So PagerDuty has an existing solution to this problem, and it's been around for a few years. It works but there's some room for improvement. The old system was called work queue. Some of my people in the audience are probably shaking their heads right now and I've got this all wrong, but sorry, I just joined the company in January I don't really know how it works. But anyway, we have this Scala service and the work you write that task to a wide row in Cassandra, the wide row means that the Q is a row in Cassandra and the task time is actually the column. Then we have another component that's polling Cassandra for new tasks and executing them using a thread poll. This is pretty simple and it works. But what happens if you have many instances trying to pull from the same queue in other words how do you distribute the tasks to available instances without doing work?
   So to solve the problem of having multiple service instances we introduce customer partitioning and the partitioning logic puts tasks into wide-row queues into Cassandra. And they there's some logic top handle service instance going down, so each instance has a primary, a secondary and a tertiary partition or instance that would work the queue and these instances would be peeking at queues to make sure the work is being done. It's kind of a big deal, like people would be paged in the middle of the night because the system is now in a degraded state. 
   We introduce some pretty complex custom partitioning logic and now our work queue library has to be aware of how many service instances there are and what happens if you need to add another one. Things get pretty hairy. So the old solution works, but it has some problems that we would like to address. So like I was talking about, adding was quite a dance. There's actually seven different steps you needed to, it's not something you want to do at 3 a.m. When Amazon decides to kick one of your boxes.
   The old solution was also quite slow, so the only way a task was executed was by pulling it from Cassandra. So we're actually hammering our Cassandra clusters with these very slow rides. And this frequently slow I/O they're all currently on the US so that latency between our Cassandra nodes is minimized. This is in large part due to work queue. We also had huge reliance on our partitioning code. That's not a good position to be in. OK, enter the new hotness. It's the same as the hold hotness, but it's on fire.
   Earlier this year, the core team built a new solution to the same problem and very originally we named it scheduler. It's not actually on fire. It's working quite well and we're going to find out how. So the new scheduler library is built in Scala, the same as the old work queue. I'm assuming most of you have some idea of what these things are, but in short Kafka is a distributed connect log and if you saw the talk yesterday about Amazon Kinesis, a lot of that is going to apply to Kafka, as well, even if the terminology is slightly different. 
   Cassandra is a database based on ideas from Amazon's big-table paper. And Akka is an actor system so if you're not familiar with actors, they are computational entities that run in parallel and communicate by sending each other messages.
   This is a diagram of the new scheduler and how it integrates into our services. Again we have some service written in Scala shown in red here. It includes a library shown in green. The service, the service has logic schedules a task by passing it into a scheduled task method and the library in turn serializes the task and queues it to Kafka. now, it's important to note here we're just serializing task metadata, we're not serializing any sort of task logic. On the other end of queue it's also been a persisted task to Cassandra, but it's also going to remain in memory.
   At the same time, the scheduler is also fetching tasks out of Cassandra in a regular basis, similar to the old work queue. And this combination of in memory tasks from Kafka and fetch tasks from Cassandra is executed by calling a task executor defined by the encompassing service so you as the library user need to define how the tasks are executed and that's how we get away with not serializing any task logic.
   So the purpose of this talk is not to explain exactly how every part of scheduler works. Instead, I would like to talk about some of the challenges typically faced when building this type of system and how we used those three technologies to help us solve those challenges.
   Everybody knows distributed systems are hard. They have many challenges. I'm going to focus on some three main areas during this talk, so dynamic loads, which is really a problem for anyone. Data center outages, and task ordering. We'll start with dynamic load. So what happens on Black Friday when everything is happening all at once and that PagerDuty we're sending out a much higher number of notifications than normal? Specifically what happens when you start scheduling more and more tasks? There's a number of components that are needed to keep up with the increased load. Generally Kafka can scale horizontally. And this works because, for a queue, which is call a topic in Kafka lingo, there are N possibly replicated partitions spread evenly across the brokers, such as each broker gets an even share of the traffic and therefore an even share of the disk I/O and etc. And what's nice is Kafka can rebalance the partitions to ensure the load load is still even. So let's take a look at how this works. We'll start with two brokers, and we have one topic, and that topic is divided into six partitions.
   Each partition is replicated, once from a leader to a follower. All the reads and writes are going to the partition leader. So in the case of a write, the leader forwards the write on to its followers. And this leader-follower stuff is important. In OK, let's say we start exhausting disk I/O in our two brokers, so we add a third to our cluster. The partitions are redistributed by Kafka, this is something you have to manually initiate. So now it's a leader for two partitions and a follower for two. Each broker only has to deal with four partitions instead of 6.
   OK, again we're going to scale out. Each broker is now a leader for one partition and a follower for one. For this to continue scaling we should have more partitions in our topic than expected brokers. This example is not going to scale further than 12 brokers, at that point, each broker is either a leader or a follower for a single partition. Adding more brokers to that cluster is not going to help. But there is also a cost to having a very large number of partitions per topic. It affect availability and latency, so this is not something you want to increase beyond reason and you can do some Googling to learn more about that.
   So that takes care of scaling Kafka. What about the service itself? Well, again we can scale horizontally and this is actually enabled by Kafka, so the consumers of a given topic, which in this case are instances grouped together and this is called a consumer group. The group's healthy consumers are tracked by Kafka through a heartbeat mechanism. The topics partitions are distributed evenly to the healthy consumers and this is a dynamic process, so if consumers die or consumers are added, the partitions are reassigned.
   So here's an example with a topic that has five partitions. With two service instances in the same consumer group, one service gets three partitions and the other gets two.
   OK, now, let's say our two service instances can't keep up with the workload so we add a third. Kafka is going to automatically reassign those partitions such that they are evenly distributed across the three instances.
   Each instance is now responsible for two partitions at maximum instead of the previous three. This is called a consumer rebalance and Kafka can do this very quickly, but typically you're actually limited by how fast your application can react to that consumer rebalance, so in this particular case, for scheduler, it's actually about 30 seconds.
   Again, note here that the scaling is not effective once your number of instances equals your number of partitions. Your instances, for them to be useful, have to be working at least one partition. So this is another thing to keep in mind when choosing the number of partitions for a topic.
   So our service instances are receiving the tasks from Kafka, but now they have to persist them somewhere so we're using Cassandra for our task persistence and like Kafka it will scale horizontally. Now I'm kind of assuming that many of you know how Cassandra scales so I'm going to go through this pretty quickly. It's not terribly interesting. The key for a Cassandra row is going to be hashed into a token into a very large range and the nodes in the cluster are assigned a range of those token, so therefore a given node will go on to a known node in the cluster. Then as you add nodes, the additional nodes are responsible for a different cluster.
   To make the math easy, we're going to say the range of token is 0 to 99. In reality it's going to be much larger.
   For example node 1 is responsible for 76 to 1, node 2 is responsible for 1 to 25. So when we insert a new row into the cluster, the token dictates which row the node will go on. In this example it was calculated to be 80, so it lives on row 1. Now, this doesn't take into account replication, a node is usually replicated meaning it's going to live on multiple nodes in the cluster. It also doesn't take into account Cassandra D nodes which a more complicated feature I won't get into right now.
   OK, so our four nodes are having trouble keeping up, we'll add a fifth node. We have to shift them around so they're still equally distributed. This is an oversimply indication that you probably wouldn't do in production, but I think it helps understand the concept. I also learned that it was very difficult to draw this in Google drawings, so I didn't want to make it more come contemplated.
   We can add machines to our various clusters to scale horizontally. But what happens when someone at Amazon trips over the internet cable? Well, we'll start with Kafka again. Our setup is to have 6 brokers evenly split across three different data centers, so you have two brokers in each data center. Now, each partition has three replicas, about one per data center. Again we want to be sure we're not going to lose any data. In Kafka 0.10, but we are still running Kafka 0.9.
   >> Now, normally if all three replicas are in synch, the rep will go to two data centers. So if a data center fails, a third of our partitions will lose their leader, but Kafka will automatically change the leader to one of the in-synch followers and all the reads and the writes are going to go to that new leader. So here's an illustration of how this works. This is a very similar example to our actual set up. The only change is the number of partitions which makes the diagram a little clearer.
   So we have three data centers, two brokers in each one, and there's a single topic which has 6 partitions. So originally, each broker is the leader for a single partition and a follower for two others.
   OK. Then we just lost data center 3. So Kafka is going to shift our partition leadership. Partitions 3 and 6 had a leader in data center 3 which is now gone, so Kafka is going to make broker 1 and 4 take on leadership of partitions 3 and 6, and the partition circle then read have changed from being followers to being leaders. The only other thing to note here is now we only have two in-synch replicas for each configuration. Writes are still going to succeed.
   OK, so Kafka makes it look easy. Now let's look at Cassandra. What happens with Cassandra when we have an entire data center go down? So we have figured Cassandra to deal with this scenario. Simply speaking we have five nodes spread evenly across three data centers and our replication factor is actually 5, which means all Cassandra nodes are eventually going to get a replica of each row. We want to be very sure that we don't lose any data.
   And we do quorum rights, which means that at least three nodes might acknowledge the right before it returns success to the application. And three machines means that we will be in at least two data centers.
   Then we do quorum reads to guarantee that we get the latest value. Now, maybe some of you are thinking that this sounds kind of slow. And it is somewhat slow but we can get away with this, because these Cassandra writes are generally not user-facing.
   So this diagram illustrates our quorum write. In the worst-case scenario, where two of your writes are to nodes in the same data center, you are still guaranteed to get a write to a node in a second data center. And again, when you hit the worst-of-case scenario, where you've just lost data center 1, along with the two nodes that have the updated value, we're still going to be OK. So we're going to do a quorum read, and we still have three nodes that will respond in the remaining data centers, but nodes 4 and 5 have old values, so do we still get the new value? And the answer is yes, and we're relying on Cassandra's policy of last write wins here.
   And yes, your clocks need to be well synched for this to happen. We do have plenty of alerting around that, and if you really start diving into the corner cases it can get hairy, so you should generally try to avoid mutating your data. So now let's talk about what happens to the service itself when a third of the nodes go down. Well, as we have seen before, Kafka will detect that those consumers are no longer healthy. It will reassign a topic partitions to the remaining healthy instances and this works for three reasons: Number one, any service instance can work any task. No. 2, the tasks are item potent, so if a task is halfway done when something reappears it can be rewritten without anything bad happening and of course we don't run our remember verse at max capacity.
   So let's say we have three data centers and a service instance in each one. The topic partitions are evenly spread out across the service instances, and notice that in data center 3, the service instance is working partition 3.
   Well, and we lost data center 3. So Kafka will detect the failure of service instance three through that heartbeat mechanism and it will repartition to service instance 1. The service is going to continue to work.
   OK, finally we need to talk about task ordering. This is a pretty tricky problem in distributed systems, but if you limit the scope of your ordering, it can get a lot easier and maybe you won't need to use vector clocks which would be great. So how did we limit the scope of our ordering problem? We have this concept of logical queues for which ordering is defined. Each task has an attribute called ordering ID which defines the logical queue that's in. For example an ordering ID might be user ID123. Now, remember that a Kafka topic is composed of many partitions, and we need to put the task into a single partition.
   So the partition ID is just a hash of the ordering ID, modded with the number of partitions and this takes care of the scheduling side. So here's an example. We have four tasks that have ordering IDs based on the user ID associated with the task. Task 1 might be an on call notification and task 3 might be an off-call notification and they're both going to the same user and let's say they're going to the same person. You're only on call for a minute and it's the best week ever.
   Ordering has to be defined for those two tasks. So the hash of the ordering ID needs the two tasks to be put into same Kafka partition. This means that they'll be executed on the same service instance or in a more complicated case task 1 will be executed, then there's a consumer rebalance and task 2 is executed elsewhere. The important part is, though, tasks 1 and task 2 or sorry, task 1 and task 3 will not be initiated on different machines at the same time.
   So on the execution side we have one service instance executing a logical queue because of course the number of logical queues is quite high. We could have millions of logical queues, and obviously we don't want to have millions of service instances. So one service instance is actually executing many logical queues, but to maintain ordering in a logical queue, if a task fails, you need to keep trying it until it succeeds and you can't be executing any tasks after it. So how can we continue to execute all of the logical queues except for the one that has a stuck task?
   So the answer in our case was an actor system initiated in Akka. You can do it in other ways, but we found this to be simple. Each service has this Akka system, which is somewhat simplified and in this diagram the ovals represent actors and the arrows represent supervision. So at the top is the topic supervisor, it receives all the incoming Kafka messages for a given service instance, now, since a service instance, ... within each partition, there are multiple logical queues, based on the tasks ordering ID. The partition executor supervises an ordering executor for each ordering ID. Now, that's potentially a lie, right, because we have millions of users and that could be a lot of different logical queues, but actors are cheap and they have something like 300 actors in overhead. The ordering executor has a queue of tasks to execute. It will take the task from the head of the queue and take a task executor for it. Now, a task executor is actually a short-lived actor. It's not going to create a task executor in the next task in the queue until the first one finishes. The task executor is going to retry the task until it succeeds. So if a task continually fails, the logical queue contained within the ordering executor stops, but the other ordering executors will continue working through their own queues, so the ordering in the system is maintained, but only where it matters. OK, maybe you're thinking enough theory, does this thing actually work? Well, it does, scheduler has been in production since March. It powers everything that happens in pager duty's on-call notification service. So if you are you've received messages saying you're going on call or off call, scheduler is the piece of software that made that happen. And it does a fair bit of work, so millions of tasks are executed each month and in addition to that, it handles some very interesting spikes. So people tend to change that you are on-call schedule on the hour, so we do large amounts of work on the hour or the half hour.
   We're also continuing to do development work on this service, which means multiple rolling deployments per day. Scheduler ensures that those deploys don't disrupt the tasks that are being worked on.
   And maybe some of you are wondering how we know all this data center outage stuff actually works. Well, our answer to that is failure Friday, and this is something we do every Friday. We purposely break our infrastructure in a very controlled and planned way. To ensure that the system can handle the failures.
   So back in May, we actually simulated a data center outage. We killed all the machines in one data center and made sure the system kept on working. It did. Then on July 19, one of our data centers actually experienced some very severe network problems which largely cut communication to our other two data centers. This is a worst-case scenario. Well, again, the service performed as expected.
   But did the new scheduler actually solve the problems from the old work queue? Now, if you remember earlier in the presentation I said that difficult manual steps were required for infrastructure changes, lots of chef work, very easy to get wrong. Now, we can actually add or remove service instances by killing or spinning up a new box. Kafka will ensure that each box will get more work and no partition will be left behind. So I can actually type it up into Slack, it will join the correct consumer group, and it will get to work.
   I also said that the old work queue had low through-put because it was frequently accessing Cassandra, with the new scheduler, Cassandra access is down 65%. And this is because Cassandra pulls for tasks at a much longer interval and it can do this because Kafka is actually feeding it more tasks. This is actually a graph of a number of Cassandra reads in production on our guinea pig service. We switched over to the new scheduler at the red arrow and you can see that the number of Cassandra reads drops quite drat dramatically.
   Lastly I said that the old work queue had complex partitioning logic that only a few people in the company understood. Well, in the new scheduler, the partitioning logic lives in Kafka, it's off the shelf. There's many people outside of our company working on it and they understand it quite well. This is a much situation to be in. So in short, yes, it did solve our problems.
   One last thing: Scheduler is an open source project, so here's the GitHub repo. I will say that using it a is a bit more complicated than just dropping it into SVT. You need solve Kafka running and Cassandra, and the docs, you know, they need some work still but we're working on it but the code is out there and you should give it a look. If this sort of stuff sounds interesting for you and you're looking for a new challenge, come talk to me or any of the other PagerDuty people here at the conference. We're always hiring and we love to chat and now is a time for questions. It's actually very hard to see you out there.
   Any questions?
   Yeah, go ahead.
    AUDIENCE:   So with your cross data replication, can you talk a little bit about this? Specifically, like how many challenges with that,.
   >> Sure, to repeat the question, you're asking about for cross-data center replication, what is our Zookeeper topology look like; is that accurate? So Zookeeper is used by Kafka, is that what you're talking about in this case?
   >> OK, so I'm not aware of anything special that we're doing, actually. Kafka is using zookeeper and its doing its thing, and yeah. Other questions? I don't see any, but yell it out if you have one. In the back?
   >> If your event starts sending text message, how do you enforce the item code of those actions in case of failure?
   >> Yeah, that's a good question, the question is if some of the tasks are site-affecting, you know, they send out a notification or something like that, how do you enforce the item potency of the task. The answer is that in some cases, and this is application dependent, but in some cases the task is actually stored somewhere, so the task has some sort of status in Cassandra and if you run it a second time, it's going to see that the task is already completed and it's not going to run it again. Does that answer your question?
   >> I think so.
   >> OK. Anyone else? No? OK. Thanks very much.
   [applause]
