   Jennie Lees.
   StrangeLoop 2016
   Live captioning by Norma Miller @whitecoatcapxg.
   
   JENNIE LEES:  All right, hello everybody, welcome back, how is the food coma? Is it good? I'm going to talk about something really exciting to wake you guys up, and that is monitoring, yeah? Cool. Hands up if you monitor your software. OK, some of you didn't put your hands up. That really scares me. Hopefully in the next 30 or 40 minutes we'll change your mind and help you understand why it's really, really cool. This isn't just about monitoring. I've been in this situation I'm sure you guys can relate where you've gone to a conference your mind is exploding with buzzwords and you get back to your office and back to your desk and you're like, I want to implement all of the things right now. And you look at all of the things you're trying to do already and you're like, well, how do I get from here to this glorious future more or less.
    I care a lot about games. I'm a gamer. We make a game conveniently. It's called League of Legends. Have you heard of the game, some people? Yeah, cool, that's awesome, I love it when people have heard of it because I have a little bit less of explaining to do. So it's a popular game. And as a game designer, 100 million is great, right, loads of people playing this thing that I love and made. As a backend engineer. I hear 100 million and I think, oh, that's a lot of things that can go wrong. And they do, right? Nothing is perfect. So a game is about 7 years old, and some key characteristics of it, it's a fairly strategic game. I like to think of it as action strategy tactics, all rolled into one. You grab four other people, mostly. You carve out and hour of your time to play and then you play this kind of big rock, paper scissors, who's going to play what, who are my friends playing, who are they playing, you get into a game and you spend an hour making decisions about what to do and because it's so complex, people take it really seriously. It's a support, an E support, right? But people get paid to play it. I wish that sort of thing had been around when I was in college. Just like just because there's professionals playing it doesn't mean normal people don't take it seriously, too, I and you spend time trying to get better at the game and get best ranking up. Which is why it really, really sucks when you lose and I don't mean your teammates are bad. I mean this. I hate this screen. Something's gone wrong with the service, right? I can't get online, I'm in the middle of a game and something went wrong. And. The players don't like this, either. They really don't like it. They make memes and YouTube video songs about our service. And like I've been in the glorious position where someone has made YouTube videos about a product I worked at they were like fan videos, it's amazing. These are not the kind of videos I want on things I work on. This is not my legacy. I've been reading too much Hamilton. So what happens behind the scenes when this stuff happens, right? So we have a this is NASA, but it's basically the same thing. And that is a room full of people, 24/7 around the globe and their job is like to detect, respond to incidents, escalate, fix things. What do they need? Information, they can't just sit there and guess, they're not psychic, I wish they were sometimes, but they're not. Monitoring comes in. We need to enable the network operations people to do their job and ideally do it with a minimal disruption of escalating to people at 4 in the morning, which can happen. So the question I like to think. My job is answering every day is how do we reliably monitor at scale. I highlight the word reliably, because we've been in this wonderful position where if the monitoring systems aren't completely reliable, why do you even have them? What's the point? If you can't trust them, then you just resort to spamming the refresh and hope no one's complaining. If they were reliable at like 80%, I guess they could kind of work. But the ideal is getting 100% reliability. So reliability, No. 1. Monitoring. It's what this talk is about. What is monitoring? I like to think of it as three layers. You've got basic systems monitoring, CPU, ram, disk space, network usage. If you're start interesting scratch, like that's the first stuff you do. 
   The next layer application monitoring, and this is more fun, right, is my application even running? Maybe not. If it is running, how is it doing? Is it doing all right? Are there some warning signs? Is it behaving badly, are there error rates? Are there response times, and you can think of the skeleton, error logs, what's with going on, less about the interior logic, because to me that is the business intelligence section and when you talk about pa game you get some really fun questions here, right? 
   Who's playing what champion, are they winning too much? Are people chatting and talking about lags too much? The spoiler, we can, it's pretty cool. There's also things like who's logging on from where, who's buying stuff? Is this normal? Is this abnormal? 
   And as soon as you get into those kinds of questions, you start going back to the network operations you're like, hey, this is stuff you should probably care about. If nobody's logging in and we're expecting peak traffic why and it's not there, that's a leave issue. That's something I care about right now and I'm trying to fix and so these lines of kind of blurry and I'll get into a little bit of how we deal with that later.
   The third part is scale. So games companies have different definitions of scale, and I come from a tech industry where like vastly varying definitions of scale so I wanted to color this in a little bit. 
   To us, scale is having a bunch of physical data centers, a bunch of metal in those data centers, a lot of things running on that metal and then just for monitoring alone, a lot of metrics, 100,000 individual data points per second being sent. 
   Obviously this isn't where we started, but this is a rough estimate of where we are today, and this is nothing in comparison to like our business intelligence pipeline itself. This is just the standard monitoring pipeline. 
   And again, when I see these kinds of numbers as an engineer, I get really scared, especially in the word this has to be reliable. It's highlighted in the front of my vision, right? These are some pretty annoying numbers to try and keep up and so this is kind of the story of how do we get there? So, life without monitoring is glorious, right? Everything is fine. You can't see the fire, you don't know it's there. It's perfect. So maybe you start out here. 
   You know, you start out with something and eh, you don't need monitoring, somebody will tell us if it's broken and you do this for a while and you think, OK, what's the most basic stuff I can do? I'm getting a bit concerned about doing nothing. So you add network, disk, Ram, CPU and standard stuff. And you're OK, I want a bit more than that. Maybe you add stuff like port checks, like it's not responding on a specific port. That to me is like monitoring 101. If you don't have that, then you're not monitoring really and the problem with stuff like that the problems with delivering a game is if it's not responding, it's dead. Something is already broken so you need more complexity than that to try to get ahead of the problem and anticipate incidents before they start hurting people's competitive experience.
   So you Google monitoring, how do I. And you find an off the shelf monitoring agent. We did this a few years back. I'm not going to talk about the specific one we used because it's kind of irrelevant to this story and I don't want to make it the bad guy. It does monitoring stuff and it does it pretty well. You end up with something that you install on every single machine and it runs a bunch of checks for you. It pushes one in every data store. We have a monitor center. 
   We realized that in the network operation center we don't want to open a tab for every data center, especially as we add more and more data centers, so we added something global on top of that. Just a little tag, keeps track of the alerts that are currently going on. Gives a little dashboard, gives everything that's happening right now, and everybody's happy, right? So happy there's unicorns and rainbows and everything is lovely and nothing ever goes wrong. Well, not quite. So one of the issues with the system is I need to know what's on the machine. To tell it what to check.
   So we're very tied to things like knowing whats on a specific piece of hardware, what role that has in our ecosystem and as we're moving towards the future, that's not really a world we're living in anymore, let's face it. We have a very diverse ecosystem. Our philosophy is do what you like, we'll figure it out. I hate to say here every programming language because I know there's going to be some that you guys work on that I'm not even aware of it. 
   And as a result you end up having the monitoring team in the middle connecting the agent on the box to tell what's running, you teach it how to talk to databases, you teach it how to talk to very specific stuff that random teams build. They have to be subject matter experts in basically everything and they have to keep up and they get paged when stuff breaks, because nobody knows who it belongs to, they're the only ones that really understand it. You end up with this blurry line between health and business intelligence and the thing that happens there is you end up in lots of arguments: 
   That becomes business intelligence, because hey, I kind of want to know how many transactions and processing an hour in the store or whatever. Well, can I keep that for like a year? Can you keep that for like five years? Well, no we're going to keep it for a week. But I want to keep it for a year. So you get in these horrible arguments and it's also like it's a monitoring system, so there are some things it didn't do. Then something like Prometheus, some of these things would not be true but they're true for the ones we chose. Things like we stored it in the data level. 
   Things like predictable alerting trends, stuff like that, not built in so we had to add other own and we had to add them in the collecting glue which is a little bit awkward for us. Things like it's a polling-based system, so every N seconds we get a data point. So we get samples. What's the problem with samples?
   They hide a lot of things. They hide spikes. They hide anomalies. They tell you useful things for like memory and disk perhaps, but they really don't give you everything you need. So we try and solve these problems. Now, having said all this, the thing worked pretty well. We got monitoring that we didn't have before. We have some information and the net operation folks are really happy because they suddenly have all this visibility. So it happened to kind of involved this for us.
   Microservices, I love this, talking of buzzwords, right? Micro-service or as we kind of call them, maybe service, just maybe a lot smaller so we started working more and more towards this microservices direction. We end in a case where one team just can't own monitoring. We don't want monitoring to be their job. We want it to be everybody's job. This is sneakily a sort of culture talk about how do you change that, how do you get people on board with thinking about something they don't really care about. It's tough. It's very tough.
   So there's two things that we can do. Kind of a hint on the screen. So we get people to change their minds. We get people to care about monitoring and not just leave it for some people to clean up after them.
   And we change the data store, take data pipeline to actually a data pipeline, rather than a monitoring pipeline. We look at things like the blurred lines, we look at things like how people want to use this data and we say well, maybe monitoring isn't special after all, maybe we can just use the same stuff we use for our big data.
   Which one do we do first? Well, one is pretty easy, I think. Changing out a technology is not too hard. Changing people's minds, that's impossible. Or nearly impossible.
   So we do that first. Just in case it failed, I guess. And we basically took a step back and said OK, we've got a micro-service ecosystem on our hands, how do we make this thing a really healthy productive micro-service environment for developers? How do we get developers all speaking at the same language rather than all shouting across the room to each other.
   And there's steps to that. First one is interoperability contracts. Seems kind of obvious when you look at it. You basically agree for everyone to talk the same way to each other, have a common language, a common way of exposing data. We came up with a spec called ambassador. The idea is if I run a service we have to expose these kind of endpoints. 
   We agreed on HTTP and JSON as format for APIs and transferring data and some of you might thing, oh, HTTP and J. H. JSON I've got a favorite thing like SOAP that's better than that, why didn't you choose that. This comes around time and time again, it doesn't matter what you agree on, it matters way way more that you agree on something and not everyone is going to be on board with it from day one. But the thing is so much more powerful than having some rogue person off using proto buff in the corner by himself.
   So the idea is I just put them in my app and I will be part of this ecosystem. We make this easier, of course. We create frameworks, we integrate with various parts of that library making it really easy for me to get metrics in my app from the moment I start writing it. And things like the status end point give us that for free, as well.
   All perfect, right? Well, not quite. One thing I learned from this experience is, it doesn't just matter what you specify. It matters what you don't, and the stuff around the edges. So you say, HTTP, JSON, some services you say HTTPS. I don't know which one so I always go to the wrong URL and I realize it's HTTPS and I have to go back and change it. So it's kind of annoying. But it is what it is. Another interesting thing is we say you have to expose these endpoints, we don't say where they have to be. So I could have whatever I like as the preceding end path. We do say you have to give us a way to expose them in a common way and so here as my little plug for swagger. Swagger is a great way of turning JSON into a web page and you can interact with the endpoints and we use it pretty heavily. So I can go to your swagger and find you which is really great.
   How do you lock down things? Not standardized yet. It's one of those things around the edges that we didn't specify quickly enough. all right.
   So we speak the same language, that's great, but we're also moving towards this world where I'm not just a service on a box. This box is not just a database server. Well, that's a poor example, it probably is. This box does a bunch of things in our back end. How do I find out what's on that box? How do you find out what's running? Kind of bread and butter of micro-service ecosystems, you guys are probably familiar with these two things, but I want to mention them. Service discovery, so I love it, we use a protocol based on Netflix's Eureka and the idea is I -- as part of my metadata, we use standard data on how you identify yourself and identify your location and that makes it super easy to interact with stuff and find it and know what it is.
   There's one problem, though. So I need to register with service covery, and I need to find it so if I don't know where service discovery. We use DNS. We take a format of your location. Tells you where you are, and of course that means the DNS has to work and you'd be surprised it's not always. You just get to choose which turtle is at the bottom, I guess. The other thing I find really, really helpful for metrics is dynamic configuration. We have what's effectively a global key value store. That's all it is, and we key based on location, application name and version, put key values in, get them back out. It is as simple as it sounds. We use git and Jenkins to push stuff in, because that gives us a lot of stuff for free. Why do I care about this so much for monitoring? This is more like application stuff now.
   It allows us to turn things on and off dynamically, turn on and off alerts, change thresholds and if you're debugging a large issue and you want more metrics and you've kind of turned them off, if you have to reglow your service to add new stuff, that redeploy might fix the issue itself. Because sometimes turning off software and back on again does actually do the trick. So you want to keep stuff as close to the problem state as possible. So if there's a problem with your dynamic configuration code you're kind of stuck. So I love this stuff.
   So we have a bunch of service, we don't care where they are because we have discovery, we know that we can talk to them a specific way. Hopefully you guys have put these two things together a little bit and the next piece will become somewhat obvious. I have a box, I have some agents on the box and my agents talk to service discovery and then talk to the metrics end point. Now, I could have skipped this slide and made things a little obfuscated and simpler, but we do local discovery on the box. Why don't we just use central service discovery? Two reasons: As the games company we sometimes have services that are somewhat ephemeral. For example, a game server that lasts while people are playing the game and pull down after the people finish the game. We also find that this gives us a very natural sharding and scaling, because if we have a central discovery and a central metrics agent then we need to figure out how to partition the space of services. This means we don't have to solve that problem yet, so that's nice, and then once again, the box, we just look lack at the metrics end point. As a service or not, control that, I can put what I like in there. Looks like this. It's just JSON. We learned that if you say, hey, developer, you have to specify the schema here, and register with this registry here and do all these things get your metrics, they probably just won't or they'll come to you have and make you do it for them and we don't want that, because that sucks, so it's just JSON, it works pretty well for us. And we change our pipeline. The only thing we changed is the box in the red. We keep the rest the same. This is important, this is the changing in flight aspect, because we don't want to retrain people on how to use stuff. We don't want a whole lot of untested code while we're running a live service. So we have to keep running the old stuff while we're also running the new stuff. We overload the same protocol that monitoring service is using so we insert into the same data store, we impersonate it and look like the same kind of data and we realize this isn't enough, we need more global services at this point, so we build a full global cache index layer that looks at everything in every data center and we use still keeping 90% of our stuff the same as it was and gradually showing developers, hey if you expose this end point this magic thing will happen in this dashboard.
   I love it when people have pictures and not words and also because it kind of illustrates the point. This uses the same pipeline. It's still running today. We haven't finished changing the engine in flight yet. And you get the ability to say, hey, this thing was spiking, I did the change, it stopped spiking, that's great. It's not just instant response, but it's also health monitoring on an ongoing basis.
   You start noticing stuff, so monitoring is this ongoing thing where you don't always know what healthy looks like and having a lot of data and a lot of dash boards gives us that color and that picture and it helps us give us a tool to communicate between various stakeholders, the service developers themselves, even product owners and management, all those people, they love tools like graphs. They tell stories.
   So we're not perfect yet. In fact we only changed one thing to a t tick, which is sad.
   Being on a monitoring team kind of cool now, you're not the afterthought janitors trying to get monitoring in at the last minute but if anything, we make the second point here worse, because I'm a developer, I can put whatever I like in my metrics, I'm going to put all the stuff I care about, and that's both health stuff and business intelligence logicky stuff, who's playing what games at what time and I get really sad because I want to keep that for years and I can't. Good example of we moved our entire game center data center from the west coast to Chicago to make sure that layers on the east coast had better latency, and we used a lot of metrics we had around that to capacity plan, to see the effect of the move but if we were thinking of monitoring as something that's fairly ephemeral, we couldn't have done that. So what happens? We run into scaling issues, my favorite things, because we're putting more and more things in the system, we have to scale it more, and systems start botoming out. They can't keep up.
   I think the service launched about 500 metrics on every single machine in every single data center all at once and that us to was a significant percentage of our traffic. And our like monitoring back end started going crazy, holy shit, I've got all this stuff and I can't keep up. So we had a communication mismatch of hey if you're going to do this, you should probably tell us so we can get ready for it and add more capacity, but we didn't have that and instead we had to scramble to keep up.
   We also end up writing the tools around these data centers and again we can't do things like predictions or trends alerting. We have to have to try to get the data out somehow. It's pretty messy and it ends up making application developers have to take on some of that work. They have to start recording rates, they have to start recording spikes themselves and exposing those into the metrics pipeline, which kind of sucks, so that's why we get on to the next step.
   Change the data out. Everything changing the backend, everything is going to be lovely, we hope. So we change the pipeline again. We keep the left-hand side the same this time. Or mostly. We get rid of the data center level storage, thank goodness, finally, we have a global store, because this is 2016 and we can do that now, and we use a bunch of things that are kind of battle tested for much more scale than we need. We use elastic search for Kafka. It's really just a buffering measure so that if if we have issues between data center and global, we can keep on the data without having to drop it on the floor and we use HTTP to get stuff in, because that's our agreed-upon protocol. This works pretty well, but of course our metrics agent does have to learn how to speak HTTP, so we have a little change there. We get a whole bunch of stuff from the switch to a big data back end, like dashboard, and etc. using ElasticSearch that we didn't previously have. We do have to retrain people but not on everything at once, just on this one thing. So how are we doing? Oh, looking good? Oh, not quite there. This is yellow, it doesn't quite show up.
   Because we're using a database, a big database solution, right, we can do things like determine whether something's a metric or not, we can change your attention periods, even access policies, we can provide a lot of tools, because ElasticSearch has a lot built in statistics are there. We get a global view. But we are still polling because we didn't change the metrics agent out. One thing at a time and polling isn't perfect. So we're still polling, what do we do? We rewrite the metrics will agent, into. We start moving more pieces out. Why? Rather than just update it, why do we totally rewrite it we actually rewrote it in Go because we wanted to make it really, really fast, really resilient. 
   We can move really fast with the code, and we can make it super-simple and it works with our cross-platform-diverse ecosystem. We can also share the code effectively so we end up writing something that's just a tool for pushing batches of metrics to our collector. So why does that need to be an agent?
   Well, it doesn't, right? It's basically a library. So we go back to our development frameworks, what we originally launched and said hey, guys, use these, you'll get your API inputs free. And we get rid of the problem where you're like snapshotting a metric inside your code. Because you're in control. So we move to push-base metrics and eventually we don't even need the agent at all. So we kind of make ourselves obsolete. So there are some caveats here. First is we haven't finished doing this yet because when you have a system that works, the resistance to changing it can be pretty high. Because it works.
   Why do you care? Like, why should I as a stakeholder care about funding a team of engineers to go solve this problem? So you can end up with a wonderful hybrid ecosystem where you have some of the second old version and some of the new version all working together and you have to work really hard to deprecate it and that's where we're at now and that's our focus is just deprecating stuff. We have to make sure that things that don't have those API endpoints built in have a way to talk to the system. We we started a bunch of side cars as we call them.
   One of the things that was mentioned yesterday is that just because you have metrics doesn't mean you have the right metrics. It's what you don't measure that's normally the thing you want to look at, right? You get paged, it's 4 in the morning, something is broken, oh, I wish I'd tracked that thing, no, I don't have it. 
   So just because you have metrics doesn't mean you're measuring the right things and so that's why we ended up with a system that made it really easy for people to add stuff in. And that has to be really important, right? You have to think about who is your customer, what are they going to need to do and how do you make life as easy as possible for them?
   The second caveat is that people have different levels of standards in perfectionism and people have different levels of caring about stuff like metrics, so if I care a lot about metrics and I've put loads of stuff in my app and loads of alerts and all kinds of things I'm probably going to get the page because my software is complaining even though it's a dependency 3 hops down the line that's broken. And you end up with this wonderful service fun time where you're trying to figure out what has actually happened just because your service has some error going on and the best way to do that is look at your architecture, look at your dependencies so you can at least be alerted with what was actually broken.
   But it's definitely something that you have to work really hard at as a culture to align around let's get ourselves all caring about this stuff, all having the same standard of alerting and make sure that just because your dependency has alerts they're not on vacation this week. And all that stuff that as an engineer should try and let someone else handle for you, but you do have to start talking about it.
   But fundamentally, changing this thing, piece by piece, helped us focus on what's important to us.
   Helping make sure that developers started changing their minds, caring about monitoring, and making sure that we didn't have some team that was just doing it for them. And making sure the network operation center had the tools to respond to things, and that means like my life as an engineer working on monitoring is a lot more fun than it would have been two years ago when we didn't have any of this stuff and it gives us a chance to hopefully the metric I care about most is the number ever YouTube videos being created about my product and I think it's gone down. I should get some hard data on that. Fundamentally, you can't change everything overnight and monitoring is awesome. Those are the two things I wanted to say today. Thank you, guys.
   [applause]
   I'm not going to attempt to take questions but I'll hanging out down there if you want to talk to me afterwards. Thank you.
   [applause]
