   Leigh Honeywell.
   StrangeLoop 2016
   Live captioning by Norma Miller @whitecoatcapxg
   
    Alex:  Good afternoon everyone. Hope everybody had a good day, saw some good stuff, knitting machines, things of that nature? I just wanted to say thanks to a lot of people. I don't have any announcements other than that, but there are tons of people that go into making this a great event so I think it's apropos to mention them here at the end before we close out the event. So I want to give a big round of applause to all the sponsors.
   [applause]
   This morning I thanked some of the other organizers, so a big thanks to them.
   [applause]
   [cheers]
   I want to give a big thanks to all the speakers. Obviously they're talking about all the great stuff that we're here to see.
   [applause]
   [cheers]
   I want to thank Norma, who's been doing the amazing captioning down here all day for three days. 
   (Mad cheers and applause).
   [laughter]
   >> She's -- she accepts [Mastercard, Visa...] 
   [laughter]
   She accepts thanks in the forms of margaritas. So I hear.
   [applause]
   [laughter]
   >> I want to thank the video crew. Ian's down here we've got several guys running the video cameras and everything. They do a fantastic job, big thanks to them.
   >> If you hadn't noticed, all the talks from yesterday are online already and we'll have you, you know, today's talks up within the next day or two and I know that they'll be working on Elm Conf and PWL by early next week. All of those will be online. So check those out. 
   And then the last thing I wanted to do was thank the Peabody staff here. The AV staff is amazing. We always have amazing support for video and audio and the back of house staff, Adrian is the main guy there but all these people are what makes us all seem seamless and flawless to all of us and so huge thanks to the back of house staff and also to the front of house staff, the people who have been helping you with the catering and the ushers and people at the door and all those people.
   >> So.
   [applause]
   [cheers]
   >> They're always fantastic and thanks to George at the dock, who is my buddy. Helps me get all the packages and all the things that I need, so he's been here for years and is always a joy to come back to. So I think that's it. Thank you to everybody who helped make this great. And we have one final great talk for you today. So our final keynote is Leigh Honeywell from Slack is she is now the manager of security response, so Leigh Honeywell.
   [applause]
   [cheers]
   
    LEIGH HONEYWELL:  Hey, everyone. I'm so excited to be here. I've heard so many great things about Strangeloop over the years and it's just such an honor to get to talk to you about some of the things I've learned about security over the years so I'm going to be talking about building secure cultures. My slides are online already so you don't have to take notes. There's lots of creative commons attributions in the notes as well as links to various resources owe check them out.
   Also, quick content note, I am going to talk about plane crashes and like pretty minor, so how they're like investigated and solved so if you're not a fan of plane crashes just a heads up. So me, I work at Slack on the security response team there I'm managing it. I used to work at Heroku, Microsoft, Symantec, I paint and I'm sometimes a cranky feminist. I was raised by a feral pack of Canadian lawyers and I think lawyers are actually important for society, I think it's actually security people that ought to be pop gizing, it's also just partly being Canadian, so sorry, that's how you know someone's Canadian, you bump into them and it if they apologize to you, they're Canadian.
   So yes get told to move fast and break things. This is one of the mantras of Silicon Valley. But then this happens. This is he tried to report a security bug to Facebook through these usually excellent bug bounty program, they misinterpreted the result, turned down the bug, he posted on Zuck's wall and then we get stuff like this. Heartbeat is a fascinating study of the security response in the open source world. A researcher at Google found the bug, went through various property channels to report it and it got shared with the Linux, a few cloud vendors, when you read through a detailed timeline of the event you start to see where the information starts to move around and there's starting to be just enough research out there that other researchers go poking and some Finnish researchers found the same bug and then the cat is out of the bag and the embargo actually broke, so it was released a Monday morning instead of a Tuesday. But I want to go back even further. This is a map of the world on July 15th, 201. This is the code red worm. It exploded a vulnerable in MS that had been fixed. The reason I know those numbers, ten years later I worked on that team, the Microsoft security response center, I handled security vulnerabilities, the thing that was pretty school there, we had this whole set of policies and practices in place where we could work with external people to take in security vulnerability information, we had all this telemetry where if someone was using malware in the wild and it would cause crashes, there's a great story in the notes, I'm not going to go into in detail that really, you would find these bugs that were being actively exploited, it would show up in telemetry and you'd be able to reverse engineer what was the vulnerability. It was this methodology cease less every second Tuesday of the month and every once in a while a second time in the month for emergency patches process of updating a billion computers around the world. So if you were running Windows in 2012, I rebooted your computer and sorry. So in that job I got to work internally within Microsoft, with developers, testers, executives, Pr, everybody, coordinating shipping patches in Windows, shipping patches in Office and working with the external researchers who were often reporting bugs to us and in the best cases we would take the lessons learned from those bugs that were reported to us, cycle them back into the development process and build more secure software in the future and at Slack we do something pretty similar. We have a bug bounty program where apparently $177,000 in bounties paid out. We've had hundreds of researchers around the world report bugs to us. 
   But still an after the fact kind of thing. So it makes me wonder, like what does it mean to build secure software before you're shipping it, right? Bounties, security response, all of this stuff is the cat's already out of the bag, your site's already owned, your desktop software that you shipped already has a bug in it. What can we do before we ship that code to make things better? So some of the things that looks like in an organization that has a healthy security culture is you have developers reaching out to the security team when they're stuck or unsure about the implementation of a feature. You have developers finding bugs in each other's code, security bugs during the code review process, you have enough tooling, testing in place that people feel like they're protected from small errors or regressions. And one of the big ones, one of the big like organizational smells of a healthy security culture is you have people saying, like fessing up when they make a mistake about security, reaching out proceed actively saying, oh, hey, I posted that code in the wrong window, I deleted it and cycled it all right, but I just wanted to let you know, also don't put your Slack token on GitHub, it makes me really sad. Has anyone had a Slack token that's been nuked that was posted on GitHub? No, OK, we nuclear them proactively. Yes? Yay! So we proactively go out and nuclear those token.
   What we're talking about here is complexity. The software that any of you know about is unimaginably or knowingly complex. We could blame the weird janky export regulation that people had to do, but in any sufficiently large codebase you're going to have those weird dark corners.
   You're going to make mistakes, we're human, we're humans, we make mistakes, you're going to introduce security bugs. One model that I like to think about in thinking about security bugs and complex systems, it's called the Swiss cheese model of accident. He talks about the latent hazards in the system, the holes in the cheese. In security, and in testing we call this bug-chaining. I first learned about this model from the book the digital doctor, hope, hype, and harm at the dawn of medicine's computer page which I particularly like because we are not just at the dawn of medicine's computer age we're really at the dawn of medicine's computer security age, too. We're all beginners at this.
   So what can we do about this? Right? We have these complex systems, they're error-prone, they're humans, they make errors, any different part of your computer system can fail. So how do we get to this solid block of cheese that errors cannot pass through? In thinking about your system you've got your components, tooling, you've got the humans involved, underlying infrastructure, cloud, somebody else's computer and each one of those is going to have holes so all that we can hope to do is make them smaller, make sure there are fewer of them and make sure they don't line up. So here are things that anyone here in the audience can do. The first thing is to go looking. As a developer there's a lot that you can do as an individual to make you learn who make your code better from a security perspective. Whether it's reading up on the latest classes of threat. Doing some at that time static analysis, reading some books or getting some training in security. But fundamentally, though it's a mind set thing. You're putting yourself in the shoes of ...
   So back when vista was under development, everybody's favorite operating system, the powers that be paid a bunch of researchers to do a full code audit and they actually ended up slipping the release of vista, because they found so many bugs. In 2011 I saw a talk given by one of these testers. Her NDA had expired and she was like, I'm going to tell all, I'm going to spill the beans, it was great. So the things that she was able to develop in doing this massive massive scale code audit there was no way that even with the pretty large security groups they had they couldn't go through every single line of code so they interviewed teams and they developed a really strong sense of security smells, so the security smells she talked about were things like people's body language, whether they seemed calm or anxious as they were talking to security people. Confidence or lack of confidence in describing the functioning of their component and so as they worked with these different developers, P MS, different folks, around the wipedos organization, they were able to say, oh, this is code, there be Dragons here. And the effectiveness was shown by just how many bugs they found. So as a self-awareness thing when you're writing code, paying attention to that gut feeling, paying attention to like this component is keeping me up at night, not ignoring that. And when you have that feeling, asking for help, whether it's from your security team, appear doing some additional research yourself, but following those hunches, developing that sense of smell for your own code and if you reach out for help within your organization and you get shot down, that's a pretty useful signal, too, maybe not a great one, but yeah, so the other thing that I love to tell people about if they haven't heard about before, is capture the flag. So there's all these hacking tournaments that people put on and I'll have some links later. It's basically jeopardy board here's the 200-point web app hacking challenge and it's a great time constrained way of learning new skills. CTF.org has a great directory of them. To get a team going, all you need is some sort of chatroom, agnostic to what kind and a Google doc. I will note that security is maybe like 10 years behind the rest of the world when it comes to having welcoming environments and often if the event has an IRC channel it will be kind of foul. It's still worth it to me, but heads up. And then, beyond just what individual things people can do, I think it's really important to think about organizational processes. 
   So more stuff I learned at Microsoft, they have this security development life cycle. It's this sophisticated, 100-plus procedure thing, compliance thing that people do. Despite its complexity, there's a lot of really interesting stuff that people can learn from it. It's all creative commons licensed and worth checking out. But it's gotten me thinking a lot, like this is great when you're shipping Office every two years, but it's less feasible for those of us in the room who ship code every day or multiple times a day. So the big thing -- the Mc given I've been chasing for a couple of years is what does this look like when you're shipping code multiple times a day? So I've been thinking, what's a minimum viable SDL? Going beyond this big process to what anyone can implement in a smaller organization? And I've come up with these four things. A risk assessment up front, getting people to think about what the security posture of a particular feature they're working on is, doing some threat modeling. It just needs to be I'm thinking through the data flows of my project up front. A checklist. And theres' a ton of different resources you can use to build those checklists on your own, whether it's the Mozilla secure coding checklist, the OS resources or Microsoft's own SDL, I can taking the parts from that that make sense to you.
   And then security analysis integrated getting those tools in place.
   So here's what it looks like at Slack. We built a self-service SDL that any team can go in and create an SDL project for a new feature that they're working on. It's been a team effort. Our head of product security, it's my friend Ari and he couldn't be here, but this is -- this is as much his work as it is mine.
   But you start talking about process, life cycles, at small organizations and people freak out. Also because, this is the section where I'm talking about Slack stuff there's a lot more emojis coming up. Just heads up. People are like, what is this process? I don't want process, you're adding friction, you're slowing me down, so you get this conflict between security and dev, but it doesn't need to be like that.
   [laughter]
   In fact, I want it to be more like this.
   [laughter]
   Some people may recognize but yeah, I want security to be like that, is that so much to ask? What does that actually look like? 
   For us it's the initial risk assessment. This is something that the Microsoft team pioneered and that's instead of the two weeks before you're shipping you're like I have to do all the security stuff, it's getting people thinking about it at the start of the project and throughout the process rather than just on that last crunch.
   So we have this little quiz that you do, it's only like 6 questions long, and various answers will dump you immediately moo high risk or you can put yourself into high risk and say yes, I'm writing SQL injection. So the questions are pretty straightforward and once you've got that risk rating it's going to go through the component survey, so the component survey allows people to opt into the things that they're doing, so if there's no mobile aspect, they uncheck the mobile part of the checklist. So the checklist looks like this, super-straightforward, and you can uncheck -- everything is checked by default at the top level and you can check the specific things, the subtopics that you need, and you can uncheck the top-level things you don't need, but we encourage people to err on the side of if you're not sure about it, just leave it checked because you can move it into the I don't need to do this column later and nobody gets mad or anything.
   So before I get to checklist, this is the part where I talk about plane crashes, so any fans of the air crash investigation show in the room? Oh, yeah, I'm glad. I'm glad I'm not the only one. So checklists are a huge part of aviation safety. And if you want to see really -- the thing that is so great about the air crash investigation show is there's terrible things that happen and then competent humans show up and they investigate them and they make processes so they never happen again, and that's why, like, air travel is safer than any other kind of travel, pretty much. So most aviation accidents result from human error and one of those root causes is often a failure to comply with checklists, but thee checklists also prevent a ton of accidents and so when we were thinking about how we were going to design this process, design this checklist, we wanted to learn from the human factor stuff of aviation safety for like, why do people avoid checklists, what could we adapt from that? So my colleague read the entire 300-page report in the use and design of checklists and we went through that and we're like, what are the parts of this that matter to security?
   So there were -- there was like some fundamental stuff of where checklists fail. It's failing to use the checklists, failing to verify the settings visually, being interrupted is a big one. So some of the things that we incorporated into hour own process, were oversight of completion of the process from the security team, the security team being involved in being tagged in at different parts of the process, making the tasks as simple as possible, and I'll go over this a little bit more, but their affirmative statements, I'm not -- yeah, affirmative statements, rather than I'm not doing the wrong thing, I am doing the right thing.
   So and then we also have the feedback cycle from the bug bounty when our checklists fail, we usually hear about it pretty quickly, which is nice and if you don't feel like reading a 300-page FAA document I definitely recommend the checklist manifesto for a slightly less dry version of a lot of the same information.
   So here's what our checklists actually look like. First of all once you've gone through the generator thing, it starts generating some Trello boards, takes a little while, and then you get a board of all of your different checklists for the different categories of security things that you need to deal with.
   Then there's a checklist for adding your Trello board to Slack. It's not the most elegant part of the process, but here's what some of the checklists actually look like, so we've got regular expressions and then we've got alert and warning messages and the alert and warning messages stuff is actually taken from the Microsoft SDL because their guidance is really good on that one and relevant to what we do and then we can have a visual overview of what's been checked off and what hasn't and most importantly, because we've connected it to Slack, we have a nice little SDL bot that tells people in your feature channel, you've connected it to the feature channel. We have a lot of channels in general. Like, (stage whisper) a lot, it's kind of bad. So here you have a couple of examples of what this looks like, so we have a checklist item and then Mary is like, hey, Josh, we're not ready to check that one off, can you leave it unchecked for now, and we'll come back to it. Or Josh is verifying with the rest of the team that this particular item has been correctly, correctly dealt with, we've disabled DTT parsing, yeah, I can check that one off. So you're bringing the security stuff to where people are actually working.
   And the reason this matters is that it moves away from that one poor tester who has to do the 120-item checklist two weeks before the launch to a collaborative enterprise, to a collaborative effort of people working together to make sure that you're shipping secure code and it also means that as teams are working on their on, getting through a particular set of checklists, if they're ever like not sure how to do things, they just tag us in, so here the team is asking Maria for help with a particular UX decision that has an impact on security. Maria brings in, hey, here's an example of how a third party does it. That would work well in this case, I think. And then the team is like, oh, yeah, that totally fits our purposes and Maria just pops out doesn't need to be in that channel all day.
   And then we have the bug bounty, people are able to file bugs themselves internally, the rules are just JSON files that live in our GitHub, so we can accept PRs, often grammar edits from the rest of the organization and we are able to move the secure development process quickly, not just move our general software development quickly. So that's what we've done as an organization, but then there's also the question of like what can -- what are the cultural factors at work? So there's a number of things in terms of the way we create cultures that affect the security of the software that we produce, and holding ourselves, those among us who are leaders in hour organizations, which I suspect is many of you, holding yourselves accountable for the things we create and the impact on culture it has, it's pretty standard blameless culture for fans of Etsy's code is craft blog. People don't want to be the one who ships the code that has XSS in it, so we need to be able to figure out, what resources do we need to get people so that they can ship secure code, whether it's time, training, or external expertise.
   So recurs center social rules. Security teams, super-often guilty. Of honestly, all of them, but let's talk about feigned surprise, right? How didn't you know that that would cause XSS, like duh!? Who's heard something like that from their security team, right? Yeah, if there's one thing that I tell here security people all the time, I like hold that sign up in their faces and say, like don't do that, it's not necessary, that that piece of feigning surprise, like it just as the rules say it has no social or educational benefit, when people feign surprise it's to make themselves feel better and making other people feel worse. It's about making other people feel smaller and that's not how you write secure software. Fundamentally writing secure software requires a level of emotional safety. I've often had the experience of trying to talk to a security person feeling like trying to pet that cat and as I say that as a security person. So there's this idea of emotional labor, that it is actually work to give gentle feedback, to be kind in the feedback that you're giving to other people. It is also emotional labor to be receiving feedback, to be open to hearing that maybe your baby is ugly. And but this isn't just like pie in the sky like let's all get along and write secure software. It's actually supported by some of the social science research into code quality, and when this comes down to it, writing secure code is writing quality code and writing quality code is writing secure code. So if we are kind in the feed back that we give, we are building a culture in which it is safe to say I don't know what I'm doing here and when people are able to say I don't know what I'm doing, they get help and then they write code that is better and that includes security. Yeah. Nobody likes being -- nobody likes their baby being called ugly, but we need to reestablish that trust as security people, because people need to be able to hear if their baby's car seat isn't plugged in or the seatbelt is Frayed as otherwise disaster, so being able to create a culture are where you don't mind saying I don't know how to create XSS. So coming back to the digital doctor, I really like this quote, because it cuts to the core of why we need to be able to have trust in each other to do security. We need to be able to talk compassionately about the ugly parts of our code, if we have any hope whatsoever of it being secure.
   And I talked a lot faster than I expected, but again, there are slides available, and, yeah, thank you very much for listening to me talk about secure software.
   [applause]
   >>
   >> So it being the last talk of the day, we're not going to do questions up on stage, but if you want to talk to me about this stuff, please feel free to come find me afterwards. Yeah.
   [applause]
   >>
   >> Thanks, everyone and that's a wrap. I hope you all had a great time at Strangeloop and you have a great time getting home, and thanks so much for coming.
   We'll be back here sept 28th through 30th next year, so keep an eye out ... ... 
