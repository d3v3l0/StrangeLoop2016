   Scott Mansfield.
   StrangeLoop 2016
   Live captioning by Norma Miller @whitecoatcapxg
   
   Oh, wow. OK, hi everybody, my name is Scott Mansfield. I'm a senior software engineer in the EVCache team at Netflix. To be clear, before I start, this has nothing to do with the open connect CDN which some people also refer to as a cache, that's a really cool interesting concept all on its own. This is not what we're talking about. This is just the Amazon Services cloud caching layer. So you've just signed up for Netflix, good decision, you get home at the end of the day, you turn on your TV, you open the Netflix app, you need to sign in with your brand new account, you choose your profile, and naturally your name is tester like everybody else. It asks you to pick a few titles that you've liked before or maybe ones that you've seen, so you pick a few, keep going, it tells you it's personalizing your experience for you. So it's building a brand new home page just to have for you and there you are, you get there, you have House of Cards, Making A Murderer, it's a fantastic documentary. You could do a search for Tom Hanks, watch Forrest Gump for the millionth time, but you settle on Narcos. Season 2 is out. And you're watching the first first part and you get to the part where .... And you're so impressed by this line you need to go rate this show 5 stars. And you're going to be just showing all of your friends all the favorite parts of all of your episodes and we're keeping track. So any guesses on how long we have to capture your attention before you go off and do something else? OK, a little bit short. It's about 90 seconds. So the faster, the smoother, the better the experience that you have, the more likely we are to keep your attention and keep you watching Netflix.
   And part of the way we do that, is by caching things on the server side. So let's take a look at that experience that you just saw, what things caches touch. You're signing up. Logging in. Choosing a profile. Picking liked videos, the personalization process, which we will look at in a little more detail later, loading the home page itself, scrolling through the home page, any AB test allocations that might be on your profile and even the box that you're seeing, there's a whole mess of other things, even, beyond that and if you look there's a bunch of these have multiple caches involved in the backend. Put another way, this is what a typical home page request looks like. Then the output from our request-tracing system. The request comes in from the left side and goes all the way down to the right and if you look at those leaf nodes on the right there's maybe half a dozen that are not in EVCache node. So I've talked about it, mentioned it, let's talk a little bit about EVCache.
   It stands for ephemeral volatile cache. It's a key value store that we run that's optimized for running on Amazon net services Ann and tuned for Netflix specific use cases, it's a distributed, sharded and replicated store. Tuneable in the same region and across multiple different regions. base highly resilient to failure. One of my colleagues called KS monkey for a service. They actually kill more of our instances than the chaos monkey do. It's topology aware. Network topology aware for faster access, linearly scalable. We can scale out the server side as much as we need to to handle the traffic. So why do we want to optimize for AWS? Like I said, it's highly dynamic, but what does that mean? Well, instances can up and disappear at any time. Zones can fail. Entire regions can become unstable for a variety of reasons. The network itself is lossy. And customer requests can bounce between regions of the same session and the simple fact of life is that failures happen and we have to be ready for them and retest them all the time so that we are ready.
   So I'll give you a sense of scale for EVCache use at Netflix. We hold hundreds of terabytes of data in cache, we do trillions of options per day, hold tens of billions of items, do millions of operations per second, millions of cross-region replications per second. We have thousands of servers, it's more than 10,000 but not quite enough to call it like tens of thousands. We have hundreds of instances per cluster up to hundreds. Hundreds of service clients that are potentially connecting to a single cluster, tens of distinct clusters. We operate across three regions and we do this with four engineers. 
   [applause]
   Yeah, so let's take a look at a single app box and a single server box and how they talk to each other. There's an application running on our cloud somewhere. The likely path is they're consuming somebody else's service client. That library would then consume our EVCache client library, and this is a Java client library, it's a jar that we vend. The server side has memcached or what looks like memcached and prana, which is a side car. It hooks into the rest of the Netflix ecosystem. The client talks directly to memcached. The side car will register with our service discovery and the client can pull that information in order to find the server. So one client, with a whole EVCache cluster here, if you look, there's three different availability zones and there's this dotted line around these boxes. That's a whole copy of data so in this picture we have three copies of data one per availability zone and a client has a connection to all of these separately. If we have many clients, all the clients have all the connections to all the boxes, so all clients connected to all servers, no clients connected to each other and no servers are talking to each other. So for reading relatively simple, we try the closest one first and there's a couple backup paths in case that one doesn't work. We can try a different node if any one of many failures happen.
   Writing is a little bit more interesting. The client writes to all three, and that's how we have multiple different copies. All clients would be writing to all of the different places that they need to write data.
   So we're going to take a look at a few different use cases going from simple to complex. This one's fairly straightforward. It's a pretty common use case, so if you have a relatively slow service and a relatively fast caches you can try the cache first. If it's not there, we can go to the service which will go to its database, typically Cassandra for us: The second one, transient data store, so thinks things like your play-back session as it's going on and you want to keep track of how you're doing and over time, we have multiple different applications talk to the cache, so you might have one start up your session, another one update your session and then do a session rollup at the very end. If you notice there's no database in this picture, it's just the cache there.
   This is a lot more interesting to me. This is our largest footprint really of caches. It's actually the primary storage mechanism for data. So we have these really large-scale precompute systems that run overnight every day to compute a brand new home page for every profile of any user. It's quite a lot of computation and it's quite a lot of data, and what they do is just write it into an EVCache cluster and the online services are reading from there, not worrying about any time whatever is written. We have a pretty good system of fall-backs, we don't have to worry about data not being there all the time. So the whole thing looks like this. So you have online services reading and the cache acts as a buffer between the two or a gateway, many other words you could use for it.
   So this is our personalization data. This is even more interesting, we have some things that are very high volume, and also need to be very high availability, so think UI strings, where if you don't have them, the user experience is pretty bad because it has no words on it.
   They would have an in memory cache and if that doesn't have data or they could do a background refresh from EVCache. If it doesn't have the data they could reach out to EVCache. If it does, they could do a background refresh. So think about whenever we're going into our peak during the day and they want to scale up the servers they could all be pulling it from the EVCache.
   And beyond that, they could even go further and have a service that owns this data with a database behind it, but most people have actually found this to be appsal, because the up time of the EVCache clusters is so good that they don't worry about it.
   So we've been looking at maybe 10,000 feet. Let's take a step back. Let's go to 3050 and take a look at what I call pipeline of personalization. So if computer home page there's not just one step. There's many different steps involved with different algorithms doing different things. You have one say compute A publishing into an EVCache cluster. Its data sources are something that we're not involved with, maybe offline hive or other data sources and then maybe a compute B. You might have a C that depends on these two and a D and maybe an E and this forms a dag of data dependencies. They might pick and choose from different parts of this in order to provide the experience that they need. So online 1 could pull from A threw D and online 2 to pull from a different set. 
   So the polyglot story at Netflix is an interesting one. It's a point of discussion right now, even but our team has decided to go ahead and solve this in our own way. Our old world or current world, really is a Java app or a Java client. It's pretty simple. But other people actually run our prana side car, as well. So we have REST API that runs on box that people can access to run the cache and we've made that remote, as well. So we have a remote HTTP API. This is still an area of active development for us so I don't have any more detail on it right now. If you want to know more, you can talk to me at the booth upstairs afterwards.
   There's some extra things on top of all of the things that I've shown you so far that make the product work at Netflix scale. The main one is our cross-region global replication. We also do cache warming for faster deployments. We have secondary indexing for point queries for debugging and we have a consistency checker that provides metrics on how consistent the caches are. All of this is powered by mutation metadata, flowing through a Kafka cluster.
   So we'll take a look at replication and the cache warming first. So replication. We have two regions here. We actually operate out of three, so this goes between like every pair and I'm just show you these two. You have an app in region A, and an app in region B and we have the cache in region A that we want to match the cache in region B. So in A could mutate that cache and send metadata from that cache through Kafka. It pull that data out of Kafka and write it across the cross-region link to what we call the cross-region proxy. And then the app in region B can see that change and of course this goes both directions or every which way between all different regions.
   The cache warming this is our steady state so you've seen this before. Without the Kafka. But in our steady state we're writing to one cache so we want to double it. There's two nodes right now, we want 4. But we can bring up a second one in parallel that's double the size and start writing to it immediately. We bring up a cache-warming app can read the data from the old one, write it into the new one until the new one looks like the old one. Then we can tear down the cache-warmer, and then tear down the old one and we're in our new steady state and very recent modifications to memcached itself have actually made it able for us to do this without the Kafka.
   This is all our code our engineers use in order to that take advantage of that. Anything that you might think to do on a memcached server, that's all our clients need to do internally.
   So let's switch gears a little bit. I've been working -- well, I've been showing you all of this higher-level design work, how the caches operate in the larger environment, let's take a deeper dive into the server side. So this is a new server for us. And we've just -- we're at the tail end of rolling this out to places that it's required. It's a project we call Moneta, it's named Moneta after the goddess of memory. It's an evolution of our server to put some data on disk. It's a cost optimization project for us and it's lowering our cost now, but also lowering the rate of increase of our cost in the future as we gain more members. This is pretty important for us, because we actually track the cost per stream start. It's a metric that our finance team tracks and something that is highly correlated if we change that it will be helpful in the future.
   And it takes advantage of our global request patterns that we have across multiple different regions.
   So server you've already seen, it's just memcached and prana, it's not very exciting. All the data is stored in Ram. This is relatively expensive compared to disk. Late last year Netflix as a company changed their architecture, they changed our architecture to be n + 1 across three regions. Meaning that any one region could go down at any time and we would be OK by shifting traffic to the other two. Any member could be served equally well from any region. In addition at the beginning of this calendar year we latched 130 new countries at the same time. So how did we optimize this? Well, we can target those personalization use case that is you saw before. Our global data means many copies so that picture that you saw with 3 copies is actually 9 globally. Three in each reasonable but our access patterns are very heavily region oriented. If you are watching Netflix in California, you are hitting the U.S. 2 reasonable. The likelihood that we see your traffic go to eu west 1 or east 1, is extremely low. So in one region or hot data is very hot and our cold data is very cold. We'll keep the hot data on RAM and the cold data on disk.
   What does our new server look like? It adds a couple of new process. There's one called rend, and another called mnemonic. We're still running memcached. The server now is a dynamic L1, L2 cache which allows it to adapt to the needs of the application at any time, without worrying about very strictly segregating data and all three of these processes are running the same protocol so we can use the same debugging tools or same load generation tools to test any one of them.
   So rend, here, is the proxy, taking external connections, and holding open connections to the internal process, memcached and mnemonic, our L1 and L2. We'll take a look at rend first, take a look at mnemonic, see the whole thing together and then we'll take a look at some performance numbers, which is probably the most exciting part for me. So any Go developers in the audience? Yes, OK. This is a Go project, you can go get it. It's actually a public GitHub repo under the Netflix org, so yeah, have fun with that. It's a high performance memcache compatible. It's written in Go mostly for it's concurrency primitives, but also because it's relatively fast for developers to get up and running and to get new code and it's very fast when it's running in production. Its purpose is to manage the L1 and L2 relationship on a single box and before the product even started we had caches that had tens of thousands of connections reading and writing. That was concurrent in this progresses from the very beginning. So taking a better look at the insides of it. It manages the connections coming in, the request orchestration between L1 and L2, communicating to the back ends and even includes our own homegrown metrics library, which might make some of you cringe, but nothing really fit our needs in the Go world because we have to integrate with our back end in Netflix. It it includes this feature where we call parallel locking, so any one server can have many parallel requests happening at the exact same time, but no one key will have two concurrent modifications which allows us to keep data integrity and keep L1 and L2 consistent.
   So mnemonic, this is our L2 piece. I say open source soon with air quotes. We don't have it there, but I want to get it on open source soon, so we'll see.
   >> And its main purpose is to map those memcached operations into RocksDB operations so in the bottom you can see RocksDB. The request comes in on top of this diagram. Run through the code that you already see in rend. And through a shim to get into a C++   world into a RocksDB library underneath to store the data on disk. We chose RocksDB for a few different reasons, mostly because when you write to RocksDB it runs into an debuffer so it's fast to write and fast to read.
   So we use FIFO compacting. It's linearly written in terms of time, so more recent files coming at the front of the queue and older files just get deleted at the end. We pin the bloom filters in indices and memory for quick access or misses, as the case may be, which does trade some L1 space, but it makes our L2 much faster which is great for us and we have separate RocksDB instances per box so that we can further decrease the latency. This is great for our precompute use cases but we're still working on figuring out how to do this for more heterogenous data.
   So if we have very fast-moving data next to -- very fast-changing data next to slow changing data, the fast-changing data can actually write enough data on the disk to push the FIFOs off the queue and which is not great the so we're working on this.
    So we have these servers already serving all of that personalization data before, remember that pipeline of personalization, all of those green EVCache nodes were this server.
   We have two ports, one for standard access, which is highly dynamic access, or actively manage data, and then we have an asynch batch port. Well, it's really a batch port, it's not asynch but it's for people who are writing data in that won't necessarily be used any time soon and it will keep the working set hot so that we don't end up blowing L1 when we're computing our personalization stuff.
   OK, the slide I've been waiting for! So this is performance and production for our most heavily loaded cache that is use Moneta right now. I measured both at the peak. All the different latencies and percentiles and things, so given all the complexity that we've added to this, you might think that it's really slow. But taking a look at the gets, our average latency is around 230 microseconds on the server side and you can see the percentiles here. The higher ones get a little high because it goes in garbage collector language. But that's really great for us and in the trough it's even faster. And the sets we're happy with this 367 microseconds average and as like I said you can see the rest of the percentiles there. But these are all great for us, if we look at client-side latencies, most people expect an answer out of us in under a millisecond, almost all the time.
   OK. I told you this is a cost optimization project. Any guesses, highest percentage savings we saw in a single cluster?
    AUDIENCE:  1,000%.
   >> Kind of impossible. 10%, a little low. OK, it was a 70% cost savings on a single cluster just because we noticed that we have a small hot set and a really large, cold dataset. And this project has been a really great one for us, and we're just nearing the end of rolling it out. So, yeah, huge success for us. All these things that you've seen, not all of them, sorry, mnemonic had the air quotes around it. EVCache, the Java client library is open source and that REST API that I mentioned earlier is also in that same repository and the rend code itself is also open source in the Netflix GitHub repo, so you can go check those out, and that's all I have.
   [applause]
   OK, I'll take any questions if anybody wants. Yes?
    AUDIENCE:  Yeah, I'm interested in the decision tree that you wound up with RocksDB. Like, can you talk about that and other stuff that you looked at?
   >> I wasn't the engineer that -- sorry, the question was the decision tree to get to RocksDB. I wasn't the engineer who made the decisions specifically to go with RocksDB. But generally it was the fastest that worked for our use case. Yeah, I have to think about it a little bit more. I'll be at the booth afterwards for a couple of hours so please come find me. Yes?
    AUDIENCE:  [inaudible]
   All right, I have we considered using Couchbase? No, not really. We talked about it before. We're happy with our own. If we need to customize it in any way, we can turn it around and get our own code. I don't believe that would be a good process for couch face. So I mean I haven't like gone into deep talks with them, but we're happy with where we are, and I don't think that we would have been able to do something like the Moneta project directly with them.
    AUDIENCE:  Have you found any limitations with the operations that are provided by memcached that if you weren't working with protocols in memcached you would be able to implement? Planned failure, for example.
   >> Right, so have we found any limitations with the commands that are available in the memcached protocol? Not really, a lot of our data is organized by user or profile or something like that. Most of the time people just want to get the whole piece of data. There's not many use cases where they just want the one array element, so it's not really necessary. Any other questions? Yes.
    AUDIENCE:  What was the project and what's the team of 4.
   >> Time scale is, I started dabbling in Go about a year ago and started hacking in Rend maybe Novemberish. I had convinced my teammates earlier this year that it was a good idea to do this project in this manner. And we are basically done rolling it into production for the largest use cases right now, so about a year total. Yeah, follow-up?
    AUDIENCE:  I was going to sap, so you left a Java-safe ecosphere with teammates know -- was it difficult to get them on board with Go where maybe some of the infrastructure was not as well tested or was it -- easy to.
   >> So how did I convince my teammates to use Go, kind of like, because we're not in the paved path anymore? It's not too hard when you make a proof of concept that's already running fast right off the bat so that's what I said it was fast and when it's compiled I had a relatively done text-base. We don't run that fast any one server, from there we can work out how to make it function on the server, we already had memcached, which was essentially a binary blob to us. We already were kind of off the paved path so it didn't really scare us so much. Plus, my teammates have been converted, I guess, to the religion of Go at this point, they seem to like the language.
   >> There will any other questions? Anybody up top have in any?
    AUDIENCE:  Was the cost in this project driven top-down by management or was that something that you came up with your team and decided to go after?
   >> OK, did the management ask us to do this to save money? Not directly. Engineers at Netflix tend to have a lot of context about how the business is doing and where it's going and what the needs are. So as a team, we looked at what our needs were, and saw this opportunity for an optimization. So we decided to just go for it, and, you know, not only do we have to convince ourselves that this is a good idea, we have to go to all these people whose data we're storing and tell them, hey we're going to change this out from under you, I hope you're OK with that, and they generally are. So it was driven mostly by the engineers, but we had backup as we were pitching this idea.
    AUDIENCE:  So did you have to do it as a side project or was it just part of your regular team duties.
   >> Started as a side project, became my main project just because we decided it was a higher priority. Anything else? Yes?
    AUDIENCE:  I was curious about the data packets that [inaudible]
   >> I'm sorry, I can't hear you. Can you speak up?
   >> Yeah, I have curious about the data app that was directly on your cache servers that applications could directly off ... ... I was wondering how that worked with [inaudible]
   >> OK, I probably didn't explain that extremely well. It's the first time I've presented that slide publicly, so our personalization process ends up creating an ad hoc data dependency dag but it's not any formalized dag that exists anywhere in any form, other than in engineers' minds of this is my dependency. So when they're writing the data out, their process writes into an EVCache cluster and those EVCache clusters are in discovery in the service discovery system, and anybody can find them at any time, so that's -- I'm not sure if I answered your question. Yeah, so anybody who wants to talk to EVCache would use the Eureka system in order to access them. Does that answer your question?
   >> Yeah, is it also keeping charts and replication and [inaudible]
   >> Well, in that diagram, there was these like single green like database-looking things, that was meant to represent a whole cluster and that's actually a globally replicated charted cluster. I saw another -- yes?
    AUDIENCE:  Any aspects of this --
   >> Any plans to extend to other cloud providers? It's open source. We accept patches. Pull requests. Not really. We -- our team is focused on solving Netflix problems. We have it in open source in the hopes that it helps other people solve their problems. It's not a primary goal for us to support the open source as an open source thing, like, but should be useful to some people. Anything else? Up top? Nothing? OK. I think that's it.
   [applause]
