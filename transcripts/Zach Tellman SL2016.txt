   Zach Tellman
   StrangeLoop 2016
   Live captioning by Norma Miller @whitecoatcapxg
   
   >> All right, I think we're going to get started. So Fitbit has a monolith, and I think that most of you can probably guess how we got here. About nine years ago, our CEO made the first commit, and we been building on it ever since and at every step that was kind of pragmatic decision, right? It was all there and for each incremental change that we made it didn't really make sense to rebuild that from scratch and so we kept building and building and now we have this million lines of code that unfortunate judge kind of sits there and we want to fix this, we want to make this better and I joined about a year ago to help with that effort and the pro edge that I was working on was one of the initial steps that you take when you are taking on a monolith which is that you start with the front-end server. Of course that's relative. You are a back end to your front end. But in the sort of entire system, there is one server which is the front-endiest, right, and that is the front-end server. So the front-end server exists so that when we start to cordon off pieces of code and we kind of pull them out into their own little piece, we can route traffic to it, right? This allows us to start to kind of thinking about these things a little bit more modularly, and the role of our server is to route the traffic, right, make sure that it goes to the right place, right service is receiving the request. It's to validate the traffic, make sure that the requests are well formed and this means that they can conform to the HTTP spec but also that they are authenticated that they aren't kind of pathological in some way. It also will shape or prioritize, traffic, if our back end is under load, is not able to handle all the requests, it's our job to figure out which are to be focused on first.
   It is positioned very advantageously with respect to the rest of our code in a way that can sort of effect great change.
   And when building a server, we really want to have four properties, sort of in order of importance, that it be transparent, stable, fast and extensible, and by transparent what I mean is that we can see what's going on inside of it while rights running in production. That means we need to have a lot of metrics. And it also needs to have a structure that allows us to map the codon to these metrics, right, if something is going wrong, we need to be able to look at the code and build a hypothesis as to what's going on quite quickly. We want it to be stable if we deploy it and everything seems to be OK, that should remain true, even if we look away for a bit. We want it to be robust to pathological requests, right? We want it to reject malformed requests, we want it to reject malicious requests, we want it to also be robust to lots of well-formed traffic, right? We want it to be able to shed load, if the back end is unable to deal with the sheer volume of requests and still stay up itself. We want it to be fast, right? We want it to add as little overhead as possible and we want it to be predictably so. If something is fast 99% of the time but slow the other 1%, that is far worse than being slightly slower on average.
   And where possible, we want it to be able to detect issues on the back end and route it around it, right? We want it to to make the system faster as a whole than it would be on its own. Lastly, we want it to be extensible. This is a project which again is very advantageously positioned. It allows us to do things that we couldn't do otherwise in other parts of the system, so it should be a Commons h it should be that other people in the company feel that they have the right ability to change to fix their problems. It needs to be easy to understand. It also means that we need to make sure that it is inflexible. It is rigid with respect to the other properties which outweigh this, right? We don't want it to be easy for them to make it unstable or slow or opaque. But for everything else we want it to be relatively flexible, right? It should be something that sort of indicates how it's meant to be changed and when we're thinking about all of these properties, the sort of key thing that we should keep in mind is that designing systems, we don't just look at what sort of the day to day traffic levels are and say we need something that can handle this, right? We need to actually think very carefully about what is the utmost craziest situation that we want to be able to handle, right? And in that crazy situation, what do we expect it to do, and what do we expect it to do if we go just over the edge, right? Does it fall over? Are we dead? Are we going and just redirecting all the traffic and these the questions that shape the code not the you know, we have X number of requests a second on average every day, and so one of the first decisions that we made is to make it an asynch server. You can have the more complex asynch model which has less cost, and we chose it here, not because it's the one true approach. We chose it because we don't control the application boundaries either now or if in the future. Right? We are providing a service to the here parts of the company and it's not our job to dictate to them what they can and can't do. How many requests they actually get to have. You know, and the deck was a little bit stacked here, because when I joined they had already written a prototype using netty and they hired me because I had years of experience using Netty.
   And so, this helps us in some ways, right? It makes us fast. Because netty is a high- performance networking layer and it makes us stable, because we're able to handle greater extremes of traffic. Unfortunately it makes us quite a bit less easy to understand, right? Stack traces are the foundation of the JVM sort of ecosystem in terms of understanding what's going on. You have the ability to do stack dumps using J stack and all of these sort of assume that your threads mean something, right, that they are a meaningful narrative about what's going on inside your code. And so that's a problem, right? Maybe a little bit less concerning is the fact that the call-back sort of model is harder to reason about, right? Someone coming in from some other team who hasn't internalized this sort of approach will find this a little bit difficult to reason about. And so the sort of first thing that we need to think about is how do we get back this sort of ease of understanding, right? The give-back is something like a stack trace, and the approach that we used was to create a finite state machine and I want to be clear because we used this sort of term to describe two different things, one of which is the formal automata theory that underlies regular expressions and then there is a another much more informal thing where we draw boxes and lines and use that as a broad spec and we're talking about the latter.
   [laughter]
   And so this is a complete finite state machine, right? We start a request and then we either succeed or we fail. This is complete. This covers all possible outcomes.
   But again, what we want to do is capture sort of the nuances, right, if something fails, we want to know why it fails, if something succeeded we want to know how it succeeded. And so we can expand this out, right? The first handling of the requests we can all out to a service which tells you whether or not the request is allowed to be in the rest of our system and if that succeeds, then we make a call to the backend of our service and if that succeeds, we succeed. So now we may have more failures and more intermediate nodes on the way to success, but then failure is sort of a broad term, right? Maybe we actually want to why we failed, how we failed. So we can talk about timeouts, right? Either the service we were calling didn't actually respond or something else happened. We don't know what, but you know, something, that's sort of the catch-all case. So you can keep on expanding this out. Further refining it and being sort of granular in terms of all the occasions, and that's what we did. And you can't quite read this I think from here, but you can see along the left is this box of the happy path. And hanging off to the right are a bunch of red boxes that represent all the ways that we can go wrong and I'm going to step through this quickly because I think it's worth understanding exactly how granular we were. So at first we have just a connection waiting for a request to come in and the request comes in or it doesn't because we time out. And once we have a connection which machine within the off service should we call. We need to get a root. If you're able to get a route. Then we try to get a connection to that machine. If we get that connection, then we go and send the request. Once we've sent the request, we wait for the response. Once we get the response, we check to see whether or not it was authorized. If it was we continue on to make a call to our backend service and we go through the same sort of song and dance again, right? We get a route to that particular machine, we get a connection to that machine, and then we send the request, but here things get a little tricky, because we've sent the request, which is just the first part, right? It's the thing that has the URI, it has the headers, we haven't set the body yet. So now we have issue where we need to send the body but it has enough information to start sending us a response and in fact, some APIs are structured such that it will take pieces, chunks of the request, and start sending back chunks of the response in line and if we don't allow these things to happen concurrently, we can deadlock, right? We're trying to send the body back up to the server. If we were to order these, right, upload the entire body, before we send the response, things could go very badly. We have to allow it, or years later, someone's going to be very confused. So over here, we have the sort of side lined upload of the body. And then back here, we're waiting back for the response, once we get the response, we forward it back to the client, we then forward the response body, and finally, we're done. Right?
   So easy. And so, again, this is a very informal thing. This has no ability to go and generate code, this is no real mapping into our codebase and so we have to sort of invent one and the way we did this is using what we call a passport, and a passport basically allows us to mark each time we advance, we stamp it each time with the state and the time stamp and that's literally what we do. Within the code we have a thing that says we are marking that we are sending the backend request body. And so this is not something which is sort of a provable implementation of this, right? Again, this is very informal, but we have now this sort of one to one mapping, right, something that we can go and trace through the diagram and sort of map it to what's going on inside the code and there are a few other sort of abstractions that we need to look at this to understand all the pieces that we fit together. We have a thing called the router which routes. Given the request, it tells us which machine that we actually sent to. This is a netty thing. We want to make sure that our requests for the various traffic route stays on that event loop. We also have a thing called a channel pool. We call it a channel pool because many calls need connections to channels.
   You see here that we have an acquire call. Given a service route and will return a future that will yield in HTTP socket. An HTTP socket is basically what you would expect, it is kind of like a TCP connection except that it speaks at a higher-level protocol. We can get the first message. The message is everything but the body. And we can write an arbitrary message into the socket.
   And I want to call out specifically here, that we have explicit mandatory parameters here that give bounds to each of these methods. It says how long they're willing to take, and in the case of the forward content, how much actual bandwidth we're willing to use. These are not optional. There's no way to opt out of this. We have a connection that represents our incoming or client connection. We have the in, we have the off. We have this thing that threads them all together and we call it a request state machine. A request state machine takes this router and the cham pool and the passport and then we hand it the initial client connection. It just goes from there. We are no longer interacting with it. And when it's finally done it tells request passport that it is done and then the request passport is then responsible for sort of generating all the metrics, right? This is sort of a critical separation here, because I think that in a lot of code, which has sort of a rich set of metrics, which you want, you end up interleaving that with all your business logic and oftentimes the metrics code is much more verbose. Because we're trying to track a bunch of things that are in flight and I think this is problems for a lot of reasons. We want to be able to look at the business code and not be thinking about all of the side channels communications that we're doing. It's also hard to verify that it's doing it right. If we have some sort of interval, which we do. We also wants how much time elapses between. -- if we were to get this wrong, if we were to measure the wrong interval, how do we know? What is our wrong baseline? It's easy to get this wrong and not know it, right? So having this something that has a data representation of this date with as being advanced to this time and being sort of able to reason to the intervals of that. It's a really helpful thing, both in terms of simplicity of code and the correctness of the metrics. So the point of this talk it to kind of give a slightly uncomfortably close look at some of the aspects of this, right? Not because necessarily we think that what we've done is amazing, and you know, transcendent and you all should take a look at this, but because I think that we don't talk about these things enough, right? We don't actually have sort of practical hands-on examples of code that actually runs in production and so I'm going to show you a bit of this code. Again with the caveat that I don't think that this is anything special and I think that there are things that are tradeoffs that I wouldn't defend, you know, to the death, but here it is.
   Here is a function called send back and request. This is what happens once we've actually received the authentication, we want to forward that request, and so it will go and it will acquire a connection to the back end and it will write it. So first we check to see if the client is actually connected. If it isn't, we just go and clean everything up and get up, because what's the point. If we are still connected, though, we go and we mark that we're going to get the route and we get the route. If there is no route, then we really can't do that and we determine that we can't proceed and bail out. We check to see whether or not the request to the client was keep alive. And once we've done that and we go and overwrite parts of the request, because there are things that are specific to our connection with the client that we don't want to transitively carry over to the backend. We don't want it to ever be anything but a keep alive connection, right? We want to reuse our connections, that's how we get better performance. Then we define a call-back, which will take a future representing the sort of connection that we're going to get to the server. If it succeeds, then we go and we mark that we've acquired the connection. We then start to write to that. We send the request, and we register another call-back, which marks the success of sending that request. If we are successful, then we mark again on the password that we have, and then we fire off these two different parts of our state machine. We recall that we are now both sending the body back up to the backend service and receiving from the backend service and we do both of those in return. If we fail to return, then we have to bail out and finally if we get a connection at all, we have to handle that and all the different ways that we could fail to get that. Having defined that call back we now mark that we're about to get the connection and then we acquire it and add a call back. And you know, the request state machine is a number of those, about 300 lines of stuff that looks roughly like this. And this code I think is not the prettiest that you've seen, right? There is a thing well publicized in the Node community called callback hell. We have a call back and we can have success or not success and this can go really deep and can have an arbitrary number of curly and closed braces. It's hard to know what kind of context you're in when you're looking at the code. Indentation is not really that helpful here. And so there exist a dozen other more libraries to try to make this better, right? You have these promise compositions, it allows you to delinearize all of these callbacks and it makes it cleaner by having a single success call back and it treats the errors as this implicit thing where if anything errors out, it will short circuit and pass that through to the bottom and this makes it simpler, undeniably, but it also sort of conflates, it mungs together all the possible errors. That runs entirely counter to our desire to understand specifically why something failed, and so if we were to try to reproduce this, we would have something that more looks like this, right, because there is an inherent complexity here, an inherent branchiness that we have to capture. We can't just sort of elide that, we can't just cover it over it because it makes the code more complex. And this is not to say that we couldn't create something that sort of looks like this, a little bit cleaner, a little bit less sort of callback hellishness. I stand before you as someone who's written so I should be the one sort of triumphantly showing you these abstractions that I wrote here that make the code very beautiful, but again, this is about 300 lines of call-back code and the abstractions if they're going to make that cleaner, then they have to be really cleaner: We doesn't want to build that big abstraction to make this tiny corner of code. It's a poor use of my time. All of a sudden there's this enormous new set of kind of terms, and concepts, and implementations that they have to understand, and so we decided to sort of leave it as is, right? The request state machine, is about 300 lines of callbacky stuff and some other code. It's messy but pragmatically so, right? We decided that that was a good call to make.
   And so thus far, I've kind of talked to you about how a single request threads through our system, right, but that's really not the entire story because we have tons of these requests flowing through at all times and we have to understand how these things work together. And there is a thing called queuing theory, it's a branch of statistics. But there are some insights that can be sort of gleaned from this and rather than talking about queuing theory, I want to tell but a pumpkin carver at a state fair and in deference to my lack of any sort of artistic ability we're going to be pretty abstract here.
   [laughter]
   So the pumpkin carvers are the X, Os are the queues and here we have a pumpkin carver doing their work. And then handle the next, right? Easy. Very reasonable to sort of think about. A problem arises, though, when someone realizes that there is no upper boundary on had you big the pumpkins can be and so someone goes and drives up in their caged forklift patiently waits in line for their turn, so then, you know, the pumpkin carver goes to carve it out and takes out all the innards and while this is happening, people keep showing up. And the problem here is not just the people who are waiting behind while the pumpkin is being carved, the problem is that once the pumpkin is gone, there's still this enormous line, right? There's still this after-effect and this can last for quite some time and how long it lasts depends on how much excess capacity is, how much utilization we're having because we have to pay down this debt now. So if our pumpkin carver was quite busy, however long that giant pumpkin took to carve, that line, the sort of excess latency that has been put into this queue will last ten times longer, which will you know, probably be the rest of the day. People will show up and they won't know why the line was so long, they won't know that there was a giant pumpkin here but they will know that it will take a long time to get their pumpkin carved. And you see this in traffic jams, right? So one way we can mitigate this is we can have many pumpkin carvers working in the queue. The other one is able to still keep things going, right, they're still able to make forward progress. The line is not going to grow unbounded behind that single carver and I want to contrast this with a different approach where we have multiple queues, right and in this case everyone has a 1 in n chance of getting behind the giant pumpkin but the other people won't, but it still means the other people are going to have a really bad time. It makes the average case better but it still isn't optimal. You'll find in many case having one person serving one queue will give you the better service.
   In our county fair, we try to get as many space spaces as possible and they tell us that we only have two spaces available, one at each end and we set up different carvers there. And we set up two lines, and so in this particular case, we can assume that we have taken to heart the fact that we won't have many consumers for a single queue and so all of our carvers are busy. One of them is finishing. They radio in that they are ready for a new pumpkin and we begin the long trek across the fairgrounds to get our pumpkin carved. And this is a problem, right? We have a situation where we have a pumpkin carver that's not doing any work. This is lost efficiency, we are not making good use of our resources here. So this is maybe optimal for the customers, for the people with the pumpkins, but it's not optimal for us. It's not optimal for our sort of money-making endeavors, and so we might sort of compromise a little bit, right? We might set up a couple of queues so that we always have at least one pumpkin on deck when we want to, you know, do some more work, right? And so now the guy with the radio at the entrance is in the job of just sending a small, bounded number of pumpkins to each of these pieces at the end of the state fair. So someone comes in, they'll be sent to one and then sent to one of the pumpkin carvers. All we're concerned about is the overall efficiency of how we're using our workers, right? In that case we love a big collection of pumpkins that we can work on. If we get too busy we can go and call in new people. All we're concerned about here is are we making good use of our pumpkin-carving resources but the state fair or carnival situation here is more nuanced, we're trying to balance these things, because there are real people waiting for us to do the work. They're going to get bored, they're going to get pissed, they're going to need bad reviews on Yelp.
   Soiree need to balance our efficiency, with our needs to make efficiency use of our resources where rear trying to make sure no one is sitting around twiddling their thumbs so it's harder, right, it's contextual, there's no one right answer here. To break down this metaphor a little bit, the different sort of collections of these pumpkin carvers, that's the backend. The entry queue, that's us, that's the front-end server, and this thing that's deciding where we want to send each pumpkin, is our load balancing algorithm, and might wonder, you know, what is the giant pumpkin metaphor for, right? You might think it's just a really expensive request. Someone uploads a gigabyte of data or something like that, but in practice it's not that simple, because we, from our position, right, where we are sort of you know, sending these pumpkins into these places we don't know the difference between a pumpkin which is really difficult to carve and a carver who's having a really difficult time. There's no difference between a pumpkin that is really hard to carve and someone who's talking on their phone the entire time or someone is taking a coffee break. So when we have these load-balancing algorithms, we need to distribute the work fairly and our goal is to send work where it can be done the most quickly, right? We're only in the business of minimizing latency. All the decisions for optimizing through-put those have been made elsewhere, those are properties of the algorithm that we've built. And sometimes the work the nodes we send the work to will be slow and we won't know why, right? Woo the engineers might be able to look into that, but we, the algorithm that is going to look into that situation does not have that sort of perspective and so we just have to deal with this as it comes. So when we talk about load-balancing algorithms, one of the most common ones that we're all sort of familiar with is round robin.
   And round robin is very simple. We have a counter that comes we increment it and we take the average of the -- the problem here is that round robin is in the business of evenly distributing request volume and so if one of our nodes is slow, we don't really care, right? We just keep on sending it the same number of requests that everyone is sending and we get this backup, right? And again, even once that node recovers there is now this backlog that is going to far outlast that sort of duration of whatever issue it was going through and round robin is very single. It uses a predictable piece of state, we can identify what it's supposed to be doing, but it amplifies issues in the system. Whenever there's a slow-down it makes it worse. So an option we can use is a least in flight strategy. It sends the next request to whichever one is handling the least so in this particular case if we have two on each of these nodes and one on the last, then we send the next request to that one. If all of them are handling an equal number of requests, then we randomly select one of them. And so the nice property here is that if one of other nodes slows down, then it just stops making forward progress and so while the other nodes around it are handling, you know, requests, are opening up capacity, we will go and we will just sort of route around, naturally, right? We'll send work to where work is actually happening. And so the two or more requests which are sort of waiting on this node, which is having a bad day, they're sort of a lost cause, right? We've sent them all right. We don't get to unring that bell. But at the very least we're going to minimize the number of requests that are going to suffer as a result of that.
   And we have a little bit more state now, and it's also a little bit harder to verify that it's doing its job because we're no longer looking at the requests per second, we're looking at what is the recurrent load on each of these machines. But it does minimize the effect of slowdowns, but there's a problem actually which is that if one of our machines is in a bad state where it's not slow but in fact very quick to error out, then this heuristic as described have had would go and start funneling all the traffic there, which amplifies again this sort of problem. We don't want to do that. And so we want to or have to, really, make this a slightly more or maybe even significantly more complicated. We have to track both the in-flight requests, we also have to track the historical failure rate of each of the nodes and to track that we use an exponentially-weighted average, which sort of biases our value of sort of success rate to the last one or two minutes.
   And we have to make sure that no matter what's going on, no matter how bad any particular node is doing we always have at least one in-flight request. We can't just ban it for life because it had a bad day and this means that we have a predictable concurrent load, but only if there weren't recent failures and again this makes it harder for us to verify that it does what we expect. But at the end of the day, it does what we want it to do. It minimizes the impact on slowdowns or outages on the backend.
   So we have an issue with our full deploys, the caches are cold, it has to jit and the round robin approach again is exactly the opposite of what we want, right? Because we keep on throwing more and more traffic at it.
   >> And so what we were seeing with the round robin, when everyone would do deploy, our 99.99th percentile was 9 seconds, we would get these big old spikes, and we'd see. When we moved to least in flight, it went down to about 1.5 seconds and we had give or take no timeouts. So this is a significant change, right this is a meaningful impact on our customers and of course this is exaggerated because we're looking at the top end of the system. If we were looking at median values, this would not be important, but this brings me to a point of metrics in general which is you improve what you measure, right? If we say that we're looking at the median latency, that's basically saying that the 49% of our customers can you know, just, you know, do whatever they want, they suck, we don't care about them, right? As long as we are going and giving the majority of our customers a good experience, like, we just don't really care, and obviously that's not true, right? But I actually much prefer the converse version of this, because I think it captures it a little bit more accurately, which is that everything you ignore will get worse, right? And this is important because it's not enough to just measure something. We can have thousands of metrics and that doesn't mean that thousands of things get consistently better in our system. What consistently gets better or at least does not consistently get worse are the metrics that we look at every day. Otherwise these will slowly and inevitably get worse. So when we were building our front-end server, we had to decide what are our key metrics, right? And it can't be latency. So the one that we decided on was overhead. Overhead and I when I showed this initial state machine, you might have noticed that some of the edges were blue. I bolded them here to make it a little more obvious. The blue transitions are where we are waiting for someone else to do something, right? We have sent a request out into the ether. And waiting for something else. Conversely, everything else that we do, if there's a connection pool sort of backup. That's time that we're adding. That is overhead that we are contributing to the request. If you're taking too long to compute something, that's overhead. So we want to keep a close eye on this, because this is ha proxy for a wide variety of things that we can do wrong. And so our key metrics are our P 99.9 overhead. And unfortunately that's not enough by itself, because we could have a low overhead because we have very low volume. But these three things together, these are what we're alerting on. These are things that we thought very carefully about what the threshold should be. We are not just adding random thresholds to dozens of metrics. It's just these three. Now we have other metrics that we keep track of but these exist so that if one of these key metrics looks weird, we can understand why, right? Having high overhead does not describe what's going on in the system. It just indicates that there's something that we should be paying attention to. And so that's really it, you know, the things that I want to, you know, have you take away from this is that if you're building a system like this, articulate what your goals are and put them in order of importance, right? As I said, you know before, we had all these things that we wanted and extensibility, right, the ability of some random person to come up and start contributing is something that we really want to allow for, but it cannot trump all the other things, it cannot trump the stability of the system and so we have to think about these things and make our tradeoffs and judge our tradeoffs on the basis of our goals and when we're building a system like this, we have to understand and describe what the extremities are. What are the things that we are willing to do, and what is our expected behavior when we sort of go over the edge?
   And choose your metrics carefully, because once you put something in production, this is what guides the evolution offer system, right? This is what informs what you change, what you tweak, what you try to improve and above all, what you allow to get worse.
   And so, that's it. Thank you.
   [applause]
   >> So I believe we have some time for questions. Yes?
    AUDIENCE:  So the passport that you were describing with the state machine, you sort of described it as being useful for like a bookkeeping purpose, to keep track of where through that sort of overall request process you are trying to get a connection, have you thought about using that also a sort of a correctness step of the actual request code [inaudible] if so.
   >> Yes, so the question was, are you using passport -- unfortunately I had to cut out quite a bit of the talk yesterday because I realized I was going to go over time. The state ma sheep is basically a mechanism for defining coverage in a more meaningful way of how many code branches are we dealing with, and so what we do is we basically set up a test harness, where we have all the back ends and have some proxies that simulate network failures and sort of tweak them that to make sure that they are following the path that we expect. And also in every log we expose that, the log passport and what the intervals were between each state and that gives us a clear understanding of what's fast, what's slow, what exactly -- you know, how did we get where we got?
    AUDIENCE:  [inaudible]
   >> So the question was, why do we roll our own, rather than using like HA proxy. So this is something that I think I meant to sort of discuss. We were talking about the goals, right? We route, we validate and we shape. HA proxy and Nginx and things like that are great for validating, there is some extensibility there that exists, but like at the end of the day, you really want, it's not a big piece of code and it makes sense to have it be very, very flexible and sort of custom fit to what you need, because it is in this powerful position with the system. An if you look at Google has the Google web server, Twitter has a Twitter front end. Netflix has zool, each of them have their own. It's not an easy thing to open source. It's a solution to fit their problem and will be developed to fit their needs. Whereas the sort of very narrow needs of a particular company are more tractable.
   >> Yes?
   >> Do you only learn on key metrics?
   >> We do, so those are where our alert thresholds are. There are other sorts of metrics that are for the surrounding systems, * and you know, but those sort of capture all of the sort of bad things, right? If there's an error that will capture all the bad things that you will normally sort of bundle and you know, latency and other things are there, so we kind of have looked at it and it does seem to capture all the things that we care about. We have run books which say if you see this, then here are the other ancillary metrics that you should look at to determine what's going on here. We don't have it in a flow chart but that's where we would eventually like it to be. It's a checklist to be sort of here's what could be going on sort of deal.
    AUDIENCE:  [inaudible]
   >> OK, so the question was, is there any reason that we would want to use Netty directly. And so we don't use Clojure at Fitbit, and so that is the main reason that you wouldn't use that. There are other Java-land sort of extensions on top of that. The short answer is it's not a big piece of code. If you're trying to write tons of asynch logic, the parts that interact directly with netty are not that many, and so at the end of the day having an additional layer of direction doesn't give you much. It costs a lot more than it, you know, sort of benefits you. Yes?
   >>
    AUDIENCE:  What do you use for service discovery?
   >> We use the Zookeeper service set mechanism. Which you know, is part of the Twitter sort of, you know, ecosystem of things.
   >> Yes?
   >>
    AUDIENCE:  What are the worst failures you've observed in production that's gone in service.
   >> So the worst failure that we've observed, I believe was, and so this was again something that I would have loved to talk about. We're going slightly over time here, if people want to go to lunch it's outside, please, I won't be offended, so the worst failure was when we actually were having out of memory. So we use off-heap memory to make memory more efficient to and it has slab reference counting which can go wrong for all the obvious reasons and so we were trying to track that pretty well, but we had this one sort of case of we were seeing in production that we hadn't tested for, and the problem was that we were using sort of out of the box metrics. And there was a metric it provided there that was called off-heat memory and so like cool, we're tracking this. It turns off it doesn't actually do what you'd think it would do, which is tell you how much off-heat memory there is. And so that value happens to be like a private field under Java nio bits and so the worst failure that we had was that after a fixed amount of time, the machines started all toppling over with out of memory exceptions and luckily we had some redundancy and one of these machines is able to handle roughly 50,000 requests per second and we caught it before enough of them fell over that it actually impacted production. And then we spent a number of very confusing days trying to figure out what was going on and examining the memory dumps to kind of understand what was going on and another part that I wanted to talk about is you really should evaluate your metrics. If you have something that's supposed to be telling you and there's some way to get external data that's telling you it's actually doing that, do that. It's going to be worthwhile. I think.
   >> Does your server do circuit breaking or -- bulkheading.
   >> We rely on the service discovery to do that for you. If a server goes and takes it the service out then we basically stop sending it traffic. All right, I don't think there's anyone. Thank you so much ...
   [applause]
